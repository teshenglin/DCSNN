{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Dp5QPfWYia_k"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from functorch import make_functional, vmap, grad, jacrev\n",
    "import functools\n",
    "\n",
    "from pyDOE import lhs\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plain(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim , h_dim , out_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear( in_dim , h_dim )\n",
    "        self.act1 =nn.Sigmoid()\n",
    "        self.ln2 = nn.Linear( h_dim , out_dim , bias=False )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.ln2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_Res(func_params, X_inner, Rf_inner):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    grad2_f = (jacrev(grad(f)))(X_inner, func_params)\n",
    "    dudX2 = (torch.diagonal(grad2_f))\n",
    "    \n",
    "    laplace = (dudX2[0] + dudX2[1])\n",
    "    \n",
    "    loss_Res = laplace - Rf_inner\n",
    "    \n",
    "    return loss_Res.flatten()\n",
    "\n",
    "\n",
    "def compute_loss_b(func_params, X_bd, U_bd):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    u_pred = f(X_bd, func_params)\n",
    "    loss_b = u_pred - U_bd\n",
    "    \n",
    "    return loss_b.flatten()\n",
    "\n",
    "def compute_loss_j(func_params, X_ij, Uj_ij):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    X_ij=X_ij.reshape(len(X_ij), 1)\n",
    "\n",
    "    ij_outer = torch.cat((X_ij[0], X_ij[1], 1.0+0.0*X_ij[0]), 0)\n",
    "    ij_inner = torch.cat((X_ij[0], X_ij[1], -1.0+0.0*X_ij[0]), 0)\n",
    "\n",
    "    u_ij_outer = f(ij_outer, func_params)\n",
    "    u_ij_inner = f(ij_inner, func_params)\n",
    "    \n",
    "    ij_pred = u_ij_outer - u_ij_inner\n",
    "    \n",
    "    loss_j = ij_pred - Uj_ij\n",
    "    \n",
    "    return loss_j.flatten()\n",
    "\n",
    "def compute_loss_normal_jump(func_params, X_ij, Normal_ij, Unj_ij):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    X_ij=X_ij.reshape(len(X_ij), 1)\n",
    "    \n",
    "    ij_outer = torch.cat((X_ij[0], X_ij[1], 1.0+0.0*X_ij[0]), 0)\n",
    "    ij_inner = torch.cat((X_ij[0], X_ij[1], -1.0+0.0*X_ij[0]), 0)\n",
    "\n",
    "    normal_x = Normal_ij[0]\n",
    "    normal_y = Normal_ij[1]\n",
    "    \n",
    "    grad_f_outer = (grad(f))(ij_outer, func_params)\n",
    "    df_outer = (grad_f_outer)\n",
    "    Normal_outer = normal_x*df_outer[0] + normal_y*df_outer[1]\n",
    "    grad_f_inner = (grad(f))(ij_inner, func_params)\n",
    "    df_inner = (grad_f_inner)\n",
    "    Normal_inner = normal_x*df_inner[0] + normal_y*df_inner[1]\n",
    "    \n",
    "    normal_jump_pred = Normal_outer - 10.0*Normal_inner\n",
    "\n",
    "    loss_normal_jump = normal_jump_pred - Unj_ij\n",
    "        \n",
    "    return loss_normal_jump.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VDhMyPhqFQuq"
   },
   "outputs": [],
   "source": [
    "# exact_u = exact solution\n",
    "def exact_u(x, y, z):\n",
    "    u1 = 0.1*(x**2+y**2)**2 - 0.01*np.log(2.0*np.sqrt(x**2+y**2))\n",
    "    un1 = np.exp(x**2+y**2)\n",
    "    eu = u1*(1.0+z)/2.0 + un1*(1.0-z)/2.0\n",
    "    return eu\n",
    "\n",
    "# rhs_f = right hand side function\n",
    "def rhs_f(x, y, z):\n",
    "    f1 = 1.6*(x**2+y**2)\n",
    "    fn1 = 4.0*(x**2+y**2+1)*np.exp(x**2+y**2)\n",
    "    rf = f1*(1.0+z)/2.0 + fn1*(1.0-z)/2.0\n",
    "    return rf\n",
    "\n",
    "# normal_u = \\nabla u \\dot n, normal derivative of u, only defined on the interface\n",
    "def normal_u(x, y, z, normal_x, normal_y):\n",
    "    u1x = 0.4*x*(x**2+y**2) - 0.01*x/(x**2+y**2)\n",
    "    u1y = 0.4*y*(x**2+y**2) - 0.01*y/(x**2+y**2)\n",
    "    u1 = normal_x*u1x + normal_y*u1y\n",
    "    un1x = 2.0*x*np.exp(x**2+y**2)\n",
    "    un1y = 2.0*y*np.exp(x**2+y**2)\n",
    "    un1 = normal_x*un1x + normal_y*un1y\n",
    "    nu = u1*(1.0+z)/2.0 + un1*(1.0-z)/2.0\n",
    "    return nu\n",
    "\n",
    "def sign_x(x, y):\n",
    "    z = 0.0*x + 1.0\n",
    "    for i in range(len(z)):\n",
    "        dist = np.sqrt(x[i]**2+y[i]**2)\n",
    "        theta = np.arctan(y[i]/x[i])\n",
    "        ri = 0.5+(1.0/7.0)*np.sin(5.0*theta)\n",
    "        if dist < ri:\n",
    "            z[i] = -1.0\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1635237654857,
     "user": {
      "displayName": "林得勝",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10265360023258000577"
     },
     "user_tz": -480
    },
    "id": "G_pNIAfNk-Ea",
    "outputId": "7a3e1dc4-762f-4187-9e3b-5fe99b294160"
   },
   "outputs": [],
   "source": [
    "# number of grid points\n",
    "N_inner = 20\n",
    "\n",
    "# Training points\n",
    "\n",
    "## X_inner: points inside the domain\n",
    "X_inner = 2.0*lhs(2, N_inner**2) - 1.0\n",
    "x = X_inner[:,0:1]\n",
    "y = X_inner[:,1:2]\n",
    "z = sign_x(x, y)\n",
    "Rf_inner = rhs_f(x, y, z)\n",
    "X_inner = np.hstack((X_inner, z))\n",
    "\n",
    "## X_bd: points at the boundary\n",
    "xx1 = np.hstack((2.0*lhs(1, N_inner) - 1.0, -1.0*np.ones((N_inner,1))))\n",
    "xx2 = np.hstack((-1.0*np.ones((N_inner,1)), 2.0*lhs(1, N_inner) - 1.0))\n",
    "xx3 = np.hstack((np.ones((N_inner,1)), 2.0*lhs(1, N_inner) - 1.0))\n",
    "xx4 = np.hstack((2.0*lhs(1, N_inner) - 1.0, np.ones((N_inner,1))))\n",
    "X_bd = np.vstack([xx1, xx2, xx3, xx4])\n",
    "X_bd = np.hstack((X_bd, 1.0+0.0*X_bd[:,0:1]))\n",
    "\n",
    "## U_bd: function values on the boundary\n",
    "x = X_bd[:,0:1]\n",
    "y = X_bd[:,1:2]\n",
    "z = 0.0*x + 1.0\n",
    "U_bd = exact_u(x, y, z)\n",
    "\n",
    "## X_ij: points on the interior interface\n",
    "theta = 2.0*np.pi*lhs(1, 4*N_inner)\n",
    "ri = 0.5+(1.0/7.0)*np.sin(5.0*theta)\n",
    "dri = (5.0/7.0)*np.cos(5.0*theta)\n",
    "x_ij = ri*np.cos(theta)\n",
    "y_ij = ri*np.sin(theta)\n",
    "X_ij = np.hstack([x_ij, y_ij])\n",
    "\n",
    "## normal vector\n",
    "normal_x = dri*np.sin(theta) + ri*np.cos(theta)\n",
    "normal_y = -dri*np.cos(theta) + ri*np.sin(theta)\n",
    "dist = np.sqrt(normal_x**2+normal_y**2)\n",
    "normal_x = normal_x/dist\n",
    "normal_y = normal_y/dist\n",
    "Normal_ij = np.hstack((normal_x, normal_y))\n",
    "\n",
    "## Uj_ij: function jump on the interior interface\n",
    "Uj_ij = exact_u(x_ij, y_ij, 0.0*x_ij+1.0) - exact_u(x_ij, y_ij, 0.0*x_ij-1.0)\n",
    "\n",
    "# beta\n",
    "beta_minus = 10.0\n",
    "## Unj_ij: normal jump on the interior interface\n",
    "Unj_ij = normal_u(x_ij, y_ij, 0.0*x_ij+1.0, normal_x, normal_y) - beta_minus*normal_u(x_ij, y_ij, 0.0*x_ij-1.0, normal_x, normal_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain(\n",
      "  (ln1): Linear(in_features=3, out_features=50, bias=True)\n",
      "  (act1): Sigmoid()\n",
      "  (ln2): Linear(in_features=50, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# single-layer model\n",
    "model = Plain(3, 50, 1).to(device)\n",
    "print(model)\n",
    "\n",
    "# Make model a functional\n",
    "func_model, func_params = make_functional(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bd = torch.from_numpy(X_bd).requires_grad_(True).double().to(device)\n",
    "U_bd = torch.from_numpy(U_bd).double().to(device)\n",
    "X_inner = torch.from_numpy(X_inner).requires_grad_(True).double().to(device)\n",
    "Rf_inner = torch.from_numpy(Rf_inner).double().to(device)\n",
    "X_ij = torch.from_numpy(X_ij).requires_grad_(True).double().to(device)\n",
    "Normal_ij = torch.from_numpy(Normal_ij).double().to(device)\n",
    "Uj_ij = torch.from_numpy(Uj_ij).double().to(device)\n",
    "Unj_ij = torch.from_numpy(Unj_ij).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_iter = 5000\n",
    "mu_update = 2 # update \\mu every mu_update iterations\n",
    "div_factor = 1.3 # \\mu <- \\mu/div_factor when loss decreases\n",
    "mul_factor = 3 # \\mu <- mul_factor*\\mu when loss incerases\n",
    "\n",
    "mu = 10**5\n",
    "loss_sum_old = 10**5\n",
    "itera = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, Loss_Res: 1.31094e+02, mu: 7.69231e+04\n",
      "Iter 101, Loss_Res: 6.18851e-01, mu: 1.54486e-01\n",
      "Iter 201, Loss_Res: 3.11313e-04, mu: 1.09171e-03\n",
      "Iter 301, Loss_Res: 5.15700e-05, mu: 4.57639e-04\n",
      "Iter 401, Loss_Res: 3.59324e-05, mu: 1.91839e-04\n",
      "Iter 501, Loss_Res: 3.23464e-05, mu: 3.13628e-04\n",
      "Iter 601, Loss_Res: 2.93951e-05, mu: 1.31471e-04\n",
      "Iter 701, Loss_Res: 2.45119e-05, mu: 2.14935e-04\n",
      "Iter 801, Loss_Res: 1.88270e-05, mu: 3.51387e-04\n",
      "Iter 901, Loss_Res: 1.53244e-05, mu: 1.47299e-04\n",
      "Iter 1001, Loss_Res: 1.35267e-05, mu: 2.40812e-04\n",
      "Iter 1101, Loss_Res: 1.22537e-05, mu: 1.00947e-04\n",
      "Iter 1201, Loss_Res: 1.12188e-05, mu: 6.43628e-04\n",
      "Iter 1301, Loss_Res: 1.00245e-05, mu: 2.69804e-04\n",
      "Iter 1401, Loss_Res: 8.71048e-06, mu: 1.13100e-04\n",
      "Iter 1501, Loss_Res: 8.22394e-06, mu: 1.84902e-04\n",
      "Iter 1601, Loss_Res: 7.89135e-06, mu: 3.02287e-04\n",
      "Iter 1701, Loss_Res: 7.58727e-06, mu: 1.26717e-04\n",
      "Iter 1801, Loss_Res: 7.27801e-06, mu: 2.07163e-04\n",
      "Iter 1901, Loss_Res: 6.40065e-06, mu: 3.38681e-04\n",
      "Iter 2001, Loss_Res: 5.27479e-06, mu: 1.41973e-04\n",
      "Iter 2101, Loss_Res: 4.53167e-06, mu: 5.95139e-05\n",
      "Iter 2201, Loss_Res: 4.17514e-06, mu: 3.79456e-04\n",
      "Iter 2301, Loss_Res: 3.93212e-06, mu: 1.59065e-04\n",
      "Iter 2401, Loss_Res: 3.69308e-06, mu: 2.60048e-04\n",
      "Iter 2501, Loss_Res: 3.49634e-06, mu: 1.09010e-04\n",
      "Iter 2601, Loss_Res: 3.32163e-06, mu: 4.56963e-05\n",
      "Iter 2701, Loss_Res: 3.17465e-06, mu: 7.47067e-05\n",
      "Iter 2801, Loss_Res: 3.02773e-06, mu: 1.22134e-04\n",
      "Iter 2901, Loss_Res: 2.88933e-06, mu: 1.99672e-04\n",
      "Iter 3001, Loss_Res: 2.76162e-06, mu: 8.37010e-05\n",
      "Iter 3101, Loss_Res: 2.67994e-06, mu: 1.36839e-04\n",
      "Iter 3201, Loss_Res: 2.59680e-06, mu: 5.73618e-05\n",
      "Iter 3301, Loss_Res: 2.51445e-06, mu: 9.37781e-05\n",
      "Iter 3401, Loss_Res: 2.44504e-06, mu: 3.93111e-05\n",
      "Iter 3501, Loss_Res: 2.26177e-06, mu: 6.42678e-05\n",
      "Iter 3601, Loss_Res: 2.12930e-06, mu: 2.69406e-05\n",
      "Iter 3701, Loss_Res: 2.06978e-06, mu: 1.71771e-04\n",
      "Iter 3801, Loss_Res: 2.02605e-06, mu: 7.20053e-05\n",
      "Iter 3901, Loss_Res: 1.98011e-06, mu: 3.01841e-05\n",
      "Iter 4001, Loss_Res: 1.90714e-06, mu: 1.26530e-05\n",
      "Iter 4101, Loss_Res: 1.84934e-06, mu: 2.06857e-05\n",
      "Iter 4201, Loss_Res: 1.82144e-06, mu: 3.38181e-05\n",
      "Iter 4301, Loss_Res: 1.80558e-06, mu: 5.52876e-05\n",
      "Iter 4401, Loss_Res: 1.79083e-06, mu: 9.03870e-05\n",
      "Iter 4501, Loss_Res: 1.77172e-06, mu: 3.78896e-05\n",
      "Iter 4601, Loss_Res: 1.73855e-06, mu: 6.19438e-05\n",
      "Iter 4701, Loss_Res: 1.68464e-06, mu: 2.59664e-05\n",
      "Iter 4801, Loss_Res: 1.64802e-06, mu: 4.24512e-05\n",
      "Iter 4901, Loss_Res: 1.62353e-06, mu: 6.94015e-05\n",
      "Iter 5001, Loss_Res: 1.60600e-06, mu: 1.13461e-04\n",
      "CPU times: user 3min 39s, sys: 6min 39s, total: 10min 18s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for step in range(LM_iter+1):\n",
    "    # Put into loss functional to get L_vec\n",
    "    L_vec_res = vmap(compute_loss_Res, (None, 0, 0))(func_params, X_inner, Rf_inner)\n",
    "    L_vec_b = vmap(compute_loss_b, (None, 0, 0))(func_params, X_bd, U_bd)\n",
    "    L_vec_j = vmap(compute_loss_j, (None, 0, 0))(func_params, X_ij, Uj_ij)\n",
    "    L_vec_nj = vmap(compute_loss_normal_jump, (None, 0, 0, 0))(func_params, X_ij, Normal_ij, Unj_ij)\n",
    "\n",
    "    L_vec_res = L_vec_res/np.sqrt(N_inner**2)\n",
    "    L_vec_b = L_vec_b/np.sqrt(4.0*N_inner)\n",
    "    L_vec_j = L_vec_j/np.sqrt(4.0*N_inner)\n",
    "    L_vec_nj = L_vec_nj/np.sqrt(4.0*N_inner)\n",
    "    loss = torch.sum(L_vec_res**2) + torch.sum(L_vec_b**2) + torch.sum(L_vec_j**2) + torch.sum(L_vec_nj**2)\n",
    "\n",
    "    # Consturct J for domain points\n",
    "    # (None, 0 ,0): func_params: no batch. data_d: batch wrt shape[0] (data[i, :]). force_value: batch wrt shape[0] (force_value[i,:])\n",
    "    \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_Res), (None, 0, 0))(func_params, X_inner, Rf_inner)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_res = g.view(len(g), -1) if cnt == 0 else torch.hstack([J_d_res, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "    \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_b), (None, 0, 0))(func_params, X_bd, U_bd)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_b = g.view(len(g), -1) if cnt == 0 else torch.hstack([J_d_b, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "        \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_j), (None, 0, 0))(func_params, X_ij, Uj_ij)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_j = g.view(len(g), -1) if cnt == 0 else torch.hstack([J_d_j, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "        \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_normal_jump), (None, 0, 0, 0))(func_params, X_ij, Normal_ij, Unj_ij)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_nj = g.contiguous().view(len(g), -1) if cnt == 0 else torch.hstack([J_d_nj, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "\n",
    "    # cat J_d and J_b into J\n",
    "    J_mat = torch.cat((J_d_res, J_d_b, J_d_j, J_d_nj))\n",
    "    L_vec = torch.cat((L_vec_res, L_vec_b, L_vec_j, L_vec_nj))\n",
    "\n",
    "    # update lambda\n",
    "    I = torch.eye((J_mat.shape[1])).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        J_product = J_mat.t()@J_mat\n",
    "        rhs = -J_mat.t()@L_vec\n",
    "        with torch.no_grad():\n",
    "            dp = torch.linalg.solve(J_product + mu*I, rhs)\n",
    "\n",
    "        # update parameters\n",
    "        cnt=0\n",
    "        for p in func_params:\n",
    "            mm=torch.Tensor([p.shape]).tolist()[0]\n",
    "            num=int(functools.reduce(lambda x,y:x*y,mm,1))\n",
    "            p+=dp[cnt:cnt+num].reshape(p.shape)\n",
    "            cnt+=num\n",
    "\n",
    "        itera += 1\n",
    "        if step % mu_update == 0:\n",
    "            #if loss_sum_check < loss_sum_old:\n",
    "            if loss < loss_sum_old:\n",
    "                mu = max(mu/div_factor, 10**(-9))\n",
    "            else:\n",
    "                mu = min(mul_factor*mu, 10**(8))\n",
    "            loss_sum_old = loss\n",
    "                \n",
    "        if step%100 == 0:\n",
    "            print(\n",
    "                    'Iter %d, Loss_Res: %.5e, mu: %.5e' % (itera, loss.item(), mu)\n",
    "                )            \n",
    "\n",
    "        if step == LM_iter or loss.item()<10**(-12):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1635237874244,
     "user": {
      "displayName": "林得勝",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10265360023258000577"
     },
     "user_tz": -480
    },
    "id": "GVMXtOuDXbHX",
    "outputId": "6fc30855-1acb-4981-d798-d175d12968d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u (absolute inf-norm): 1.003719e-03\n",
      "Error u (absolute 2-norm): 2.287779e-04\n"
     ]
    }
   ],
   "source": [
    "# number of test points\n",
    "N_test = 12800\n",
    "\n",
    "# Error on the interior points\n",
    "X_inn = 2.0*lhs(2, N_test) - 1.0\n",
    "xx = X_inn[:,0:1]\n",
    "yy = X_inn[:,1:2]\n",
    "zz = sign_x(xx, yy)\n",
    "Exact_test = exact_u(xx, yy, zz)\n",
    "X_inn = np.hstack((X_inn, zz))\n",
    "X_inn_torch = torch.tensor(X_inn).double().to(device)\n",
    "u_pred = func_model(func_params, X_inn_torch).detach().cpu().numpy()\n",
    "\n",
    "error = np.absolute(u_pred - Exact_test)\n",
    "\n",
    "error_u_inf = np.linalg.norm(error, np.inf)\n",
    "print('Error u (absolute inf-norm): %e' % (error_u_inf))\n",
    "error_u_2 = np.linalg.norm(error,2)/np.sqrt(N_test)\n",
    "print('Error u (absolute 2-norm): %e' % (error_u_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZcfc6qh2ZTFS9fkQJlNfp",
   "collapsed_sections": [],
   "name": "SINet_poisson_2D_square_ellipse_double.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
