{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Dp5QPfWYia_k"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from functorch import make_functional, vmap, grad, jacrev\n",
    "import functools\n",
    "\n",
    "from pyDOE import lhs\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plain(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim , h_dim , out_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear( in_dim , h_dim )\n",
    "        self.act1 =nn.Sigmoid()\n",
    "        self.ln2 = nn.Linear( h_dim , out_dim , bias=False )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.ln2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_Res(func_params, X_inner, Rf_inner):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    grad2_f = (jacrev(grad(f)))(X_inner, func_params)\n",
    "    dudX2 = (torch.diagonal(grad2_f))\n",
    "    \n",
    "    laplace = (dudX2[0] + dudX2[1])\n",
    "    \n",
    "    loss_Res = laplace - Rf_inner\n",
    "\n",
    "    return loss_Res.flatten()\n",
    "\n",
    "\n",
    "def compute_loss_b(func_params, X_bd, U_bd):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    u_pred = f(X_bd, func_params)\n",
    "    loss_b = u_pred - U_bd\n",
    "        \n",
    "    return loss_b.flatten()\n",
    "\n",
    "def compute_loss_j(func_params, X_ij, Uj_ij):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    X_ij=X_ij.reshape(len(X_ij), 1)\n",
    "\n",
    "    ij_outer = torch.cat((X_ij[0], X_ij[1], 1.0+0.0*X_ij[0]), 0)\n",
    "    ij_inner = torch.cat((X_ij[0], X_ij[1], -1.0+0.0*X_ij[0]), 0)\n",
    "\n",
    "    u_ij_outer = f(ij_outer, func_params)\n",
    "    u_ij_inner = f(ij_inner, func_params)\n",
    "    \n",
    "    ij_pred = u_ij_outer - u_ij_inner\n",
    "    \n",
    "    loss_j = ij_pred - Uj_ij\n",
    "        \n",
    "    return loss_j.flatten()\n",
    "\n",
    "def compute_loss_normal_jump(func_params, X_ij, Normal_ij, Unj_ij):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    X_ij=X_ij.reshape(len(X_ij), 1)\n",
    "    \n",
    "    ij_outer = torch.cat((X_ij[0], X_ij[1], 1.0+0.0*X_ij[0]), 0)\n",
    "    ij_inner = torch.cat((X_ij[0], X_ij[1], -1.0+0.0*X_ij[0]), 0)\n",
    "\n",
    "    normal_x = Normal_ij[0]\n",
    "    normal_y = Normal_ij[1]\n",
    "    \n",
    "    grad_f_outer = (grad(f))(ij_outer, func_params)\n",
    "    df_outer = (grad_f_outer)\n",
    "    Normal_outer = normal_x*df_outer[0] + normal_y*df_outer[1]\n",
    "    grad_f_inner = (grad(f))(ij_inner, func_params)\n",
    "    df_inner = (grad_f_inner)\n",
    "    Normal_inner = normal_x*df_inner[0] + normal_y*df_inner[1]\n",
    "    \n",
    "    normal_jump_pred = 1.0e-3*Normal_outer - Normal_inner\n",
    "\n",
    "    loss_normal_jump = normal_jump_pred - Unj_ij\n",
    "        \n",
    "    return loss_normal_jump.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VDhMyPhqFQuq"
   },
   "outputs": [],
   "source": [
    "# exact_u = exact solution\n",
    "def exact_u(x, y, z):\n",
    "    u1 = np.sin(x)*np.sin(y)\n",
    "    un1 = np.exp(x+y)\n",
    "    eu = u1*(1.0+z)/2.0 + un1*(1.0-z)/2.0\n",
    "    return eu\n",
    "\n",
    "# rhs_f = right hand side function\n",
    "def rhs_f(x, y, z):\n",
    "    f1 = -2.0*np.sin(x)*np.sin(y)\n",
    "    fn1 = 2.0*np.exp(x+y)\n",
    "    rf = f1*(1.0+z)/2.0 + fn1*(1.0-z)/2.0\n",
    "    return rf\n",
    "\n",
    "# normal_u = \\nabla u \\dot n, normal derivative of u, only defined on the interface\n",
    "def normal_u(x, y, z):\n",
    "    normal = normal_vector(x, y)\n",
    "    normal_x = normal[:, 0:1]\n",
    "    normal_y = normal[:, 1:2]\n",
    "    u1x = np.cos(x)*np.sin(y)\n",
    "    u1y = np.sin(x)*np.cos(y)\n",
    "    u1 = normal_x*u1x + normal_y*u1y\n",
    "    un1x = np.exp(x+y)\n",
    "    un1y = np.exp(x+y)\n",
    "    un1 = normal_x*un1x + normal_y*un1y\n",
    "    nu = u1*(1.0+z)/2.0 + un1*(1.0-z)/2.0\n",
    "    return nu\n",
    "\n",
    "# normal_vector = normal vector, only defined on the interface\n",
    "def normal_vector(x, y):\n",
    "    dist = np.sqrt((25.0*x)**2 + (4.0*y)**2)\n",
    "    normal_x = 25.0*x/dist\n",
    "    normal_y = 4.0*y/dist\n",
    "    normal = np.hstack((normal_x, normal_y))\n",
    "    return normal\n",
    "\n",
    "def sign_x(x, y):\n",
    "    z = 0.0*x + 1.0\n",
    "    for i in range(len(z)):\n",
    "        dist = np.sqrt((x[i]/0.2)**2+(y[i]/0.5)**2)\n",
    "        if dist < 1.0:\n",
    "            z[i] = -1.0\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1635237654857,
     "user": {
      "displayName": "林得勝",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10265360023258000577"
     },
     "user_tz": -480
    },
    "id": "G_pNIAfNk-Ea",
    "outputId": "7a3e1dc4-762f-4187-9e3b-5fe99b294160"
   },
   "outputs": [],
   "source": [
    "# number of grid points\n",
    "N_inner = 8\n",
    "\n",
    "# Training points\n",
    "\n",
    "## X_inner: points inside the domain\n",
    "X_inner = 2.0*lhs(2, N_inner**2) - 1.0\n",
    "x = X_inner[:,0:1]\n",
    "y = X_inner[:,1:2]\n",
    "z = sign_x(x, y)\n",
    "Rf_inner = rhs_f(x, y, z)\n",
    "X_inner = np.hstack((X_inner, z))\n",
    "\n",
    "## X_bd: points at the boundary, totally 4*N_inner points\n",
    "xx1 = np.hstack((2.0*lhs(1, N_inner) - 1.0, -1.0*np.ones((N_inner,1))))\n",
    "xx2 = np.hstack((-1.0*np.ones((N_inner,1)), 2.0*lhs(1, N_inner) - 1.0))\n",
    "xx3 = np.hstack((np.ones((N_inner,1)), 2.0*lhs(1, N_inner) - 1.0))\n",
    "xx4 = np.hstack((2.0*lhs(1, N_inner) - 1.0, np.ones((N_inner,1))))\n",
    "X_bd = np.vstack([xx1, xx2, xx3, xx4])\n",
    "X_bd = np.hstack((X_bd, 1.0+0.0*X_bd[:,0:1]))\n",
    "\n",
    "## U_bd: function values on the boundary, totally 4*N_inner points\n",
    "x = X_bd[:,0:1]\n",
    "y = X_bd[:,1:2]\n",
    "z = 0.0*x + 1.0\n",
    "U_bd = exact_u(x, y, z)\n",
    "\n",
    "## X_ij: points on the interior interface, totally 4*N_inner points\n",
    "theta = 2.0*np.pi*lhs(1, 4*N_inner)\n",
    "x_ij = 0.2*np.cos(theta)\n",
    "y_ij = 0.5*np.sin(theta)\n",
    "X_ij = np.hstack([x_ij, y_ij])\n",
    "\n",
    "## normal vector\n",
    "Normal_ij = normal_vector(x_ij, y_ij)\n",
    "\n",
    "## Uj_ij: function jump on the interior interface, totally 4*N_inner points\n",
    "Uj_ij = exact_u(x_ij, y_ij, 0.0*x_ij+1.0) - exact_u(x_ij, y_ij, 0.0*x_ij-1.0)\n",
    "\n",
    "# beta_plus\n",
    "beta_plus = 1.0e-3\n",
    "## Unj_ij: normal jump on the interior interface, totally 4*N_inner points\n",
    "Unj_ij = beta_plus*normal_u(x_ij, y_ij, 0.0*x_ij+1.0) - normal_u(x_ij, y_ij, 0.0*x_ij-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain(\n",
      "  (ln1): Linear(in_features=3, out_features=20, bias=True)\n",
      "  (act1): Sigmoid()\n",
      "  (ln2): Linear(in_features=20, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# single-layer model\n",
    "model = Plain(3, 20, 1).to(device)\n",
    "print(model)\n",
    "\n",
    "# Make model a functional\n",
    "func_model, func_params = make_functional(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bd = torch.from_numpy(X_bd).requires_grad_(True).double().to(device)\n",
    "U_bd = torch.from_numpy(U_bd).double().to(device)\n",
    "X_inner = torch.from_numpy(X_inner).requires_grad_(True).double().to(device)\n",
    "Rf_inner = torch.from_numpy(Rf_inner).double().to(device)\n",
    "X_ij = torch.from_numpy(X_ij).requires_grad_(True).double().to(device)\n",
    "Normal_ij = torch.from_numpy(Normal_ij).double().to(device)\n",
    "Uj_ij = torch.from_numpy(Uj_ij).double().to(device)\n",
    "Unj_ij = torch.from_numpy(Unj_ij).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_iter = 2000\n",
    "mu_update = 2 # update \\mu every mu_update iterations\n",
    "div_factor = 1.3 # \\mu <- \\mu/div_factor when loss decreases\n",
    "mul_factor = 3 # \\mu <- mul_factor*\\mu when loss incerases\n",
    "\n",
    "mu = 10**5\n",
    "loss_sum_old = 10**5\n",
    "itera = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, Loss_Res: 4.76299e+00, mu: 7.69231e+04\n",
      "Iter 101, Loss_Res: 1.10157e-01, mu: 1.54486e-01\n",
      "Iter 201, Loss_Res: 9.28054e-07, mu: 1.84041e-05\n",
      "Iter 301, Loss_Res: 6.56735e-08, mu: 7.71488e-06\n",
      "Iter 401, Loss_Res: 5.55418e-09, mu: 8.29236e-07\n",
      "Iter 501, Loss_Res: 9.42543e-10, mu: 3.47610e-07\n",
      "Iter 601, Loss_Res: 4.35711e-10, mu: 1.45716e-07\n",
      "Iter 701, Loss_Res: 2.62125e-10, mu: 2.38223e-07\n",
      "Iter 801, Loss_Res: 1.76082e-10, mu: 3.89460e-07\n",
      "Iter 901, Loss_Res: 1.28750e-10, mu: 1.63259e-07\n",
      "Iter 1001, Loss_Res: 9.38681e-11, mu: 2.66904e-07\n",
      "Iter 1101, Loss_Res: 6.91377e-11, mu: 1.11884e-07\n",
      "Iter 1201, Loss_Res: 4.94772e-11, mu: 1.82914e-07\n",
      "Iter 1301, Loss_Res: 3.30718e-11, mu: 7.66763e-08\n",
      "Iter 1401, Loss_Res: 2.09861e-11, mu: 1.25354e-07\n",
      "Iter 1501, Loss_Res: 1.41786e-11, mu: 5.25477e-08\n",
      "Iter 1601, Loss_Res: 9.39132e-12, mu: 2.20276e-08\n",
      "Iter 1701, Loss_Res: 6.39773e-12, mu: 3.60119e-08\n",
      "Iter 1801, Loss_Res: 4.61513e-12, mu: 1.50959e-08\n",
      "Iter 1901, Loss_Res: 3.72708e-12, mu: 2.46796e-08\n",
      "Iter 2001, Loss_Res: 3.14004e-12, mu: 1.03455e-08\n",
      "CPU times: user 37.5 s, sys: 1min 26s, total: 2min 3s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for step in range(LM_iter+1):\n",
    "    # Put into loss functional to get L_vec\n",
    "    L_vec_res = vmap(compute_loss_Res, (None, 0, 0))(func_params, X_inner, Rf_inner)\n",
    "    L_vec_b = vmap(compute_loss_b, (None, 0, 0))(func_params, X_bd, U_bd)\n",
    "    L_vec_j = vmap(compute_loss_j, (None, 0, 0))(func_params, X_ij, Uj_ij)\n",
    "    L_vec_nj = vmap(compute_loss_normal_jump, (None, 0, 0, 0))(func_params, X_ij, Normal_ij, Unj_ij)\n",
    "\n",
    "    L_vec_res = L_vec_res/np.sqrt(N_inner**2)\n",
    "    L_vec_b = L_vec_b/np.sqrt(4.0*N_inner)\n",
    "    L_vec_j = L_vec_j/np.sqrt(4.0*N_inner)\n",
    "    L_vec_nj = L_vec_nj/np.sqrt(4.0*N_inner)\n",
    "    loss = torch.sum(L_vec_res**2) + torch.sum(L_vec_b**2) + torch.sum(L_vec_j**2) + torch.sum(L_vec_nj**2)\n",
    "\n",
    "    # Consturct J for domain points\n",
    "    # (None, 0 ,0): func_params: no batch. data_d: batch wrt shape[0] (data[i, :]). force_value: batch wrt shape[0] (force_value[i,:])\n",
    "    \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_Res), (None, 0, 0))(func_params, X_inner, Rf_inner)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_res = g.view(len(g), -1) if cnt == 0 else torch.hstack([J_d_res, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "    \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_b), (None, 0, 0))(func_params, X_bd, U_bd)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_b = g.view(len(g), -1) if cnt == 0 else torch.hstack([J_d_b, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "        \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_j), (None, 0, 0))(func_params, X_ij, Uj_ij)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_j = g.view(len(g), -1) if cnt == 0 else torch.hstack([J_d_j, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "        \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_normal_jump), (None, 0, 0, 0))(func_params, X_ij, Normal_ij, Unj_ij)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_nj = g.contiguous().view(len(g), -1) if cnt == 0 else torch.hstack([J_d_nj, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "\n",
    "    # cat J_d and J_b into J\n",
    "    J_mat = torch.cat((J_d_res, J_d_b, J_d_j, J_d_nj))\n",
    "    L_vec = torch.cat((L_vec_res, L_vec_b, L_vec_j, L_vec_nj))\n",
    "\n",
    "    # update lambda\n",
    "    I = torch.eye((J_mat.shape[1])).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        J_product = J_mat.t()@J_mat\n",
    "        rhs = -J_mat.t()@L_vec\n",
    "        with torch.no_grad():\n",
    "            dp = torch.linalg.solve(J_product + mu*I, rhs)\n",
    "\n",
    "        # update parameters\n",
    "        cnt=0\n",
    "        for p in func_params:\n",
    "            mm=torch.Tensor([p.shape]).tolist()[0]\n",
    "            num=int(functools.reduce(lambda x,y:x*y,mm,1))\n",
    "            p+=dp[cnt:cnt+num].reshape(p.shape)\n",
    "            cnt+=num\n",
    "\n",
    "        itera += 1\n",
    "        if step % mu_update == 0:\n",
    "            #if loss_sum_check < loss_sum_old:\n",
    "            if loss < loss_sum_old:\n",
    "                mu = max(mu/div_factor, 10**(-9))\n",
    "            else:\n",
    "                mu = min(mul_factor*mu, 10**(8))\n",
    "            loss_sum_old = loss\n",
    "                \n",
    "        if step%100 == 0:\n",
    "            print(\n",
    "                    'Iter %d, Loss_Res: %.5e, mu: %.5e' % (itera, loss.item(), mu)\n",
    "                )            \n",
    "\n",
    "        if step == LM_iter or loss.item()<10**(-12):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1635237874244,
     "user": {
      "displayName": "林得勝",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10265360023258000577"
     },
     "user_tz": -480
    },
    "id": "GVMXtOuDXbHX",
    "outputId": "6fc30855-1acb-4981-d798-d175d12968d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u (absolute inf-norm): 2.636954e-05\n",
      "Error u (absolute 2-norm): 9.376879e-07\n"
     ]
    }
   ],
   "source": [
    "# number of test points\n",
    "N_test = 12800\n",
    "\n",
    "# Error on the interior points\n",
    "X_inn = 2.0*lhs(2, N_test) - 1.0\n",
    "xx = X_inn[:,0:1]\n",
    "yy = X_inn[:,1:2]\n",
    "zz = sign_x(xx, yy)\n",
    "Exact_test = exact_u(xx, yy, zz)\n",
    "X_inn = np.hstack((X_inn, zz))\n",
    "X_inn_torch = torch.tensor(X_inn).double().to(device)\n",
    "u_pred = func_model(func_params, X_inn_torch).detach().cpu().numpy()\n",
    "\n",
    "error = np.absolute(u_pred - Exact_test)\n",
    "\n",
    "error_u_inf = np.linalg.norm(error, np.inf)\n",
    "print('Error u (absolute inf-norm): %e' % (error_u_inf))\n",
    "error_u_2 = np.linalg.norm(error,2)/np.sqrt(N_test)\n",
    "print('Error u (absolute 2-norm): %e' % (error_u_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZcfc6qh2ZTFS9fkQJlNfp",
   "collapsed_sections": [],
   "name": "SINet_poisson_2D_square_ellipse_double.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
