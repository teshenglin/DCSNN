{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Dp5QPfWYia_k"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pyDOE import lhs\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plain(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim , h_dim , out_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear( in_dim , h_dim )\n",
    "        self.act1 =nn.Sigmoid()\n",
    "        self.ln2 = nn.Linear( h_dim , out_dim , bias=False )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.ln2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, X_inner, Rf_inner, X_bd, U_bd, X_ij, Normal_ij, Uj_ij, Unj_ij):\n",
    "    \n",
    "# loss_bd: boundary condition\n",
    "    bd_pred = model(X_bd)\n",
    "    loss_bd = torch.mean((bd_pred - U_bd) ** 2)\n",
    "\n",
    "# loss_res: system residual\n",
    "    inner_pred = model(X_inner)\n",
    "    dudX = torch.autograd.grad(\n",
    "        inner_pred, X_inner, \n",
    "        grad_outputs=torch.ones_like(inner_pred), \n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "        )[0] # u_x u_y\n",
    "    dudX_xX = torch.autograd.grad(\n",
    "        dudX[:,0], X_inner, \n",
    "        grad_outputs=torch.ones_like(dudX[:,0]), \n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "        )[0] # u_xx u_xy\n",
    "    dudX_yX = torch.autograd.grad(\n",
    "        dudX[:,1], X_inner, \n",
    "        grad_outputs=torch.ones_like(dudX[:,1]), \n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "        )[0] # u_yx u_yy\n",
    "    laplace = (dudX_xX[:,0] + dudX_yX[:,1]) #u_xx + u_yy\n",
    "    loss_res = torch.mean((laplace - Rf_inner.squeeze(1)) ** 2)\n",
    "\n",
    "# loss_jump: jump condition\n",
    "    ij_outer = torch.cat([X_ij[:,0:2], 1.0+0.0*X_ij[:,0:1]], dim=1)\n",
    "    ij_inner = torch.cat([X_ij[:,0:2], -1.0+0.0*X_ij[:,0:1]], dim=1)\n",
    "\n",
    "    u_ij_outer = model(ij_outer)\n",
    "\n",
    "    ux_ij_outer = torch.autograd.grad(\n",
    "        u_ij_outer, ij_outer, \n",
    "        grad_outputs=torch.ones_like(u_ij_outer),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    normal_x = Normal_ij[:, 0:1]\n",
    "    normal_y = Normal_ij[:, 1:2]\n",
    "    \n",
    "    Normal_outer = normal_x*ux_ij_outer[:,0:1] + normal_y*ux_ij_outer[:,1:2]\n",
    "\n",
    "    u_ij_inner = model(ij_inner)\n",
    "\n",
    "    ux_ij_inner = torch.autograd.grad(\n",
    "        u_ij_inner, ij_inner, \n",
    "        grad_outputs=torch.ones_like(u_ij_inner),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    Normal_inner = normal_x*ux_ij_inner[:,0:1] + normal_y*ux_ij_inner[:,1:2]\n",
    "\n",
    "    jump_pred = u_ij_outer - u_ij_inner\n",
    "    loss_jump = torch.mean((jump_pred - Uj_ij)**2)\n",
    "\n",
    "    normal_jump_pred = 1.0e-3*Normal_outer - Normal_inner\n",
    "    loss_normal_jump = torch.mean((normal_jump_pred - Unj_ij)**2)\n",
    "\n",
    "    loss = loss_bd + loss_res + loss_jump + loss_normal_jump\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VDhMyPhqFQuq"
   },
   "outputs": [],
   "source": [
    "# exact_u = exact solution\n",
    "def exact_u(x, y, z):\n",
    "    u1 = np.sin(x)*np.sin(y)\n",
    "    un1 = np.exp(x+y)\n",
    "    eu = u1*(1.0+z)/2.0 + un1*(1.0-z)/2.0\n",
    "    return eu\n",
    "\n",
    "# rf_u = right hand side function\n",
    "def rf_u(x, y, z):\n",
    "    f1 = -2.0*np.sin(x)*np.sin(y)\n",
    "    fn1 = 2.0*np.exp(x+y)\n",
    "    rf = f1*(1.0+z)/2.0 + fn1*(1.0-z)/2.0\n",
    "    return rf\n",
    "\n",
    "# normal_u = \\nabla u \\dot n, normal derivative of u, only defined on the interface\n",
    "def normal_u(x, y, z):\n",
    "    normal = normal_vector(x, y)\n",
    "    normal_x = normal[:, 0:1]\n",
    "    normal_y = normal[:, 1:2]\n",
    "    u1x = np.cos(x)*np.sin(y)\n",
    "    u1y = np.sin(x)*np.cos(y)\n",
    "    u1 = normal_x*u1x + normal_y*u1y\n",
    "    un1x = np.exp(x+y)\n",
    "    un1y = np.exp(x+y)\n",
    "    un1 = normal_x*un1x + normal_y*un1y\n",
    "    nu = u1*(1.0+z)/2.0 + un1*(1.0-z)/2.0\n",
    "    return nu\n",
    "\n",
    "# normal_vector = normal vector, only defined on the interface\n",
    "def normal_vector(x, y):\n",
    "    dist = np.sqrt((25.0*x)**2 + (4.0*y)**2)\n",
    "    normal_x = 25.0*x/dist\n",
    "    normal_y = 4.0*y/dist\n",
    "    normal = np.hstack((normal_x, normal_y))\n",
    "    return normal\n",
    "\n",
    "def sign_x(x, y):\n",
    "    z = 0.0*x + 1.0\n",
    "    for i in range(len(z)):\n",
    "        dist = np.sqrt((x[i]/0.2)**2+(y[i]/0.5)**2)\n",
    "        if dist < 1.0:\n",
    "            z[i] = -1.0\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BcCubMPzsfB1"
   },
   "outputs": [],
   "source": [
    "def chebyshev_first_kind(dim,n):\n",
    "  a_new=(1.0/n)-1.0\n",
    "  X=[]\n",
    "  x=[]\n",
    "  X=(np.mgrid[[slice(None,n),]*dim])\n",
    "  XX=np.cos(np.pi*(X+0.5)/n)\n",
    "  for i in range(len(X)):\n",
    "    x.append(np.array(XX[i].tolist()).reshape(n**dim,1))\n",
    "  return np.hstack(np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1635237654857,
     "user": {
      "displayName": "林得勝",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10265360023258000577"
     },
     "user_tz": -480
    },
    "id": "G_pNIAfNk-Ea",
    "outputId": "7a3e1dc4-762f-4187-9e3b-5fe99b294160"
   },
   "outputs": [],
   "source": [
    "# number of grid points\n",
    "N_inner = 8\n",
    "\n",
    "# Training points\n",
    "\n",
    "## X_inner: points inside the domain, totally (N_inner-1)**2 points\n",
    "X_inner = chebyshev_first_kind(2, 8)\n",
    "x = X_inner[:,0:1]\n",
    "y = X_inner[:,1:2]\n",
    "z = sign_x(x, y)\n",
    "X_inner = np.hstack((X_inner, z))\n",
    "Rf_inner = rf_u(x, y, z)\n",
    "\n",
    "## X_bd: points on the boundary, totally 4*N_inner points\n",
    "cheby_point = chebyshev_first_kind(1, N_inner)\n",
    "dumy_one = np.ones((N_inner,1))\n",
    "xx1 = np.hstack((cheby_point, -1.0*dumy_one, dumy_one))\n",
    "xx2 = np.hstack((-1.0*dumy_one, cheby_point, dumy_one))\n",
    "xx3 = np.hstack((dumy_one, cheby_point, dumy_one))\n",
    "xx4 = np.hstack((cheby_point, dumy_one, dumy_one))\n",
    "X_bd = np.vstack([xx1, xx2, xx3, xx4])\n",
    "\n",
    "## U_bd: function values on the boundary, totally 4*N_inner points\n",
    "x = X_bd[:,0:1]\n",
    "y = X_bd[:,1:2]\n",
    "z = 0.0*x + 1.0\n",
    "U_bd = exact_u(x, y, z)\n",
    "\n",
    "## X_ij: points on the interior interface, totally 4*N_inner points\n",
    "theta = 2.0*np.pi*lhs(1, 4*N_inner)\n",
    "x_ij = 0.2*np.cos(theta)\n",
    "y_ij = 0.5*np.sin(theta)\n",
    "X_ij = np.hstack([x_ij, y_ij])\n",
    "\n",
    "## normal vector\n",
    "Normal_ij = normal_vector(x_ij, y_ij)\n",
    "\n",
    "## Uj_ij: function jump on the interior interface, totally 4*N_inner points\n",
    "Uj_ij = exact_u(x_ij, y_ij, 0.0*x_ij+1.0) - exact_u(x_ij, y_ij, 0.0*x_ij-1.0)\n",
    "\n",
    "# beta_plus\n",
    "beta_plus = 1.0e-3\n",
    "## Unj_ij: normal jump on the interior interface, totally 4*N_inner points\n",
    "Unj_ij = beta_plus*normal_u(x_ij, y_ij, 0.0*x_ij+1.0) - normal_u(x_ij, y_ij, 0.0*x_ij-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain(\n",
      "  (ln1): Linear(in_features=3, out_features=20, bias=True)\n",
      "  (act1): Sigmoid()\n",
      "  (ln2): Linear(in_features=20, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# one-hidden-layer model\n",
    "num_neuron = 20\n",
    "\n",
    "model = Plain(3, num_neuron, 1).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bd = torch.from_numpy(X_bd).requires_grad_(True).double().to(device)\n",
    "U_bd = torch.from_numpy(U_bd).double().to(device)\n",
    "X_inner = torch.from_numpy(X_inner).requires_grad_(True).double().to(device)\n",
    "Rf_inner = torch.from_numpy(Rf_inner).double().to(device)\n",
    "X_ij = torch.from_numpy(X_ij).requires_grad_(True).double().to(device)\n",
    "Normal_ij = torch.from_numpy(Normal_ij).double().to(device)\n",
    "Uj_ij = torch.from_numpy(Uj_ij).double().to(device)\n",
    "Unj_ij = torch.from_numpy(Unj_ij).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam_iter = 50000 # stop when iter > Adam_iter\n",
    "update_lr_1 = 25000  # change adam lr from 0.01 to 0.001 at update_lr_1 iteration\n",
    "update_lr_2 = 40000  # change adam lr from 0.001 to 0.0001 at update_lr_2 iteration\n",
    "\n",
    "optimizerAdam = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "itera = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, LossAdam: 3.48960e+00\n",
      "Iter 1000, LossAdam: 1.88640e-03\n",
      "Iter 2000, LossAdam: 3.82671e-04\n",
      "Iter 3000, LossAdam: 1.98789e-04\n",
      "Iter 4000, LossAdam: 1.69757e-04\n",
      "Iter 5000, LossAdam: 1.50282e-04\n",
      "Iter 6000, LossAdam: 1.31747e-04\n",
      "Iter 7000, LossAdam: 1.15357e-04\n",
      "Iter 8000, LossAdam: 1.50192e-04\n",
      "Iter 9000, LossAdam: 8.94163e-05\n",
      "Iter 10000, LossAdam: 6.87667e-05\n",
      "Iter 11000, LossAdam: 5.77072e-05\n",
      "Iter 12000, LossAdam: 5.36947e-05\n",
      "Iter 13000, LossAdam: 4.46540e-05\n",
      "Iter 14000, LossAdam: 1.02603e-04\n",
      "Iter 15000, LossAdam: 3.64675e-05\n",
      "Iter 16000, LossAdam: 4.54100e-05\n",
      "Iter 17000, LossAdam: 6.31230e-05\n",
      "Iter 18000, LossAdam: 2.84872e-05\n",
      "Iter 19000, LossAdam: 2.65990e-05\n",
      "Iter 20000, LossAdam: 2.89604e-05\n",
      "Iter 21000, LossAdam: 2.36716e-05\n",
      "Iter 22000, LossAdam: 2.36570e-05\n",
      "Iter 23000, LossAdam: 2.15019e-05\n",
      "Iter 24000, LossAdam: 2.06253e-05\n",
      "change learning rate to 0.001\n",
      "Iter 25000, LossAdam: 1.98653e-05\n",
      "Iter 26000, LossAdam: 1.97544e-05\n",
      "Iter 27000, LossAdam: 1.95793e-05\n",
      "Iter 28000, LossAdam: 1.92955e-05\n",
      "Iter 29000, LossAdam: 1.87262e-05\n",
      "Iter 30000, LossAdam: 1.75534e-05\n",
      "Iter 31000, LossAdam: 1.63425e-05\n",
      "Iter 32000, LossAdam: 1.54730e-05\n",
      "Iter 33000, LossAdam: 1.42701e-05\n",
      "Iter 34000, LossAdam: 1.34861e-05\n",
      "Iter 35000, LossAdam: 1.26762e-05\n",
      "Iter 36000, LossAdam: 1.74394e-05\n",
      "Iter 37000, LossAdam: 1.13797e-05\n",
      "Iter 38000, LossAdam: 1.08367e-05\n",
      "Iter 39000, LossAdam: 1.03424e-05\n",
      "change learning rate to 0.0001\n",
      "Iter 40000, LossAdam: 9.87745e-06\n",
      "Iter 41000, LossAdam: 9.82094e-06\n",
      "Iter 42000, LossAdam: 9.72930e-06\n",
      "Iter 43000, LossAdam: 9.58147e-06\n",
      "Iter 44000, LossAdam: 9.34643e-06\n",
      "Iter 45000, LossAdam: 8.99828e-06\n",
      "Iter 46000, LossAdam: 8.64523e-06\n",
      "Iter 47000, LossAdam: 8.31005e-06\n",
      "Iter 48000, LossAdam: 8.03240e-06\n",
      "Iter 49000, LossAdam: 7.70341e-06\n",
      "Iter 50000, LossAdam: 7.42754e-06\n",
      "CPU times: user 1min 21s, sys: 119 ms, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.train()\n",
    "for step in range(Adam_iter+1):\n",
    "    if step == update_lr_1:\n",
    "        print('change learning rate to 0.001')\n",
    "        for g in optimizerAdam.param_groups:\n",
    "            g['lr'] = 0.001\n",
    "    if step == update_lr_2:\n",
    "        print('change learning rate to 0.0001')\n",
    "        for g in optimizerAdam.param_groups:\n",
    "            g['lr'] = 0.0001\n",
    "              \n",
    "    # Backward and optimize\n",
    "    optimizerAdam.zero_grad()\n",
    "        \n",
    "    lossAdam = loss(model, X_inner, Rf_inner, X_bd, U_bd, X_ij, Normal_ij, Uj_ij, Unj_ij)\n",
    "        \n",
    "    if step % 1000 == 0:\n",
    "        print('Iter %d, LossAdam: %.5e' % (itera, lossAdam.item()))\n",
    "\n",
    "    lossAdam.backward(retain_graph = True)\n",
    "        \n",
    "    if step == Adam_iter:\n",
    "        break\n",
    "\n",
    "    optimizerAdam.step()\n",
    "    itera += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1635237874244,
     "user": {
      "displayName": "林得勝",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10265360023258000577"
     },
     "user_tz": -480
    },
    "id": "GVMXtOuDXbHX",
    "outputId": "6fc30855-1acb-4981-d798-d175d12968d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u (absolute inf-norm): 8.561671e-04\n",
      "Error u (absolute 2-norm): 2.118665e-04\n"
     ]
    }
   ],
   "source": [
    "# number of test points\n",
    "N_test = 12800\n",
    "\n",
    "# Error on the interior points\n",
    "X_inn = 2.0*lhs(2, N_test) - 1.0\n",
    "xx = X_inn[:,0:1]\n",
    "yy = X_inn[:,1:2]\n",
    "zz = sign_x(xx, yy)\n",
    "Exact_test = exact_u(xx, yy, zz)\n",
    "X_inn = np.hstack((X_inn, zz))\n",
    "X_inn_torch = torch.tensor(X_inn).double().to(device)\n",
    "u_pred = model(X_inn_torch).detach().cpu().numpy()\n",
    "\n",
    "error = np.absolute(u_pred - Exact_test)\n",
    "\n",
    "error_u_inf = np.linalg.norm(error, np.inf)\n",
    "print('Error u (absolute inf-norm): %e' % (error_u_inf))\n",
    "error_u_2 = np.linalg.norm(error,2)/np.sqrt(N_test)\n",
    "print('Error u (absolute 2-norm): %e' % (error_u_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZcfc6qh2ZTFS9fkQJlNfp",
   "collapsed_sections": [],
   "name": "SINet_poisson_2D_square_ellipse_double.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
