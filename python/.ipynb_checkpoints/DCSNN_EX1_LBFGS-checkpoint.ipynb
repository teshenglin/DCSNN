{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Dp5QPfWYia_k"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pyDOE import lhs\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plain(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim , h_dim , out_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear( in_dim , h_dim )\n",
    "        self.act1 =nn.Sigmoid()\n",
    "        self.ln2 = nn.Linear( h_dim , out_dim , bias=False )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.ln2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, X_inner, Rf_inner, X_bd, U_bd, X_ij, Normal_ij, Uj_ij, Unj_ij):\n",
    "    \n",
    "# loss_bd: boundary condition\n",
    "    bd_pred = model(X_bd)\n",
    "    loss_bd = torch.mean((bd_pred - U_bd) ** 2)\n",
    "\n",
    "# loss_res: system residual\n",
    "    inner_pred = model(X_inner)\n",
    "    dudX = torch.autograd.grad(\n",
    "        inner_pred, X_inner, \n",
    "        grad_outputs=torch.ones_like(inner_pred), \n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "        )[0] # u_x u_y\n",
    "    dudX_xX = torch.autograd.grad(\n",
    "        dudX[:,0], X_inner, \n",
    "        grad_outputs=torch.ones_like(dudX[:,0]), \n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "        )[0] # u_xx u_xy\n",
    "    dudX_yX = torch.autograd.grad(\n",
    "        dudX[:,1], X_inner, \n",
    "        grad_outputs=torch.ones_like(dudX[:,1]), \n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "        )[0] # u_yx u_yy\n",
    "    laplace = (dudX_xX[:,0] + dudX_yX[:,1]) #u_xx + u_yy\n",
    "    loss_res = torch.mean((laplace - Rf_inner.squeeze(1)) ** 2)\n",
    "\n",
    "# loss_jump: jump condition\n",
    "    ij_outer = torch.cat([X_ij[:,0:2], 1.0+0.0*X_ij[:,0:1]], dim=1)\n",
    "    ij_inner = torch.cat([X_ij[:,0:2], -1.0+0.0*X_ij[:,0:1]], dim=1)\n",
    "\n",
    "    u_ij_outer = model(ij_outer)\n",
    "\n",
    "    ux_ij_outer = torch.autograd.grad(\n",
    "        u_ij_outer, ij_outer, \n",
    "        grad_outputs=torch.ones_like(u_ij_outer),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    normal_x = Normal_ij[:, 0:1]\n",
    "    normal_y = Normal_ij[:, 1:2]\n",
    "    \n",
    "    Normal_outer = normal_x*ux_ij_outer[:,0:1] + normal_y*ux_ij_outer[:,1:2]\n",
    "\n",
    "    u_ij_inner = model(ij_inner)\n",
    "\n",
    "    ux_ij_inner = torch.autograd.grad(\n",
    "        u_ij_inner, ij_inner, \n",
    "        grad_outputs=torch.ones_like(u_ij_inner),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    Normal_inner = normal_x*ux_ij_inner[:,0:1] + normal_y*ux_ij_inner[:,1:2]\n",
    "\n",
    "    jump_pred = u_ij_outer - u_ij_inner\n",
    "    loss_jump = torch.mean((jump_pred - Uj_ij)**2)\n",
    "\n",
    "    normal_jump_pred = 1.0e-3*Normal_outer - Normal_inner\n",
    "    loss_normal_jump = torch.mean((normal_jump_pred - Unj_ij)**2)\n",
    "\n",
    "    loss = loss_bd + loss_res + loss_jump + loss_normal_jump\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VDhMyPhqFQuq"
   },
   "outputs": [],
   "source": [
    "# exact_u = exact solution\n",
    "def exact_u(x, y, z):\n",
    "    u1 = np.sin(x)*np.sin(y)\n",
    "    un1 = np.exp(x+y)\n",
    "    eu = u1*(1.0+z)/2.0 + un1*(1.0-z)/2.0\n",
    "    return eu\n",
    "\n",
    "# rf_u = right hand side function\n",
    "def rf_u(x, y, z):\n",
    "    f1 = -2.0*np.sin(x)*np.sin(y)\n",
    "    fn1 = 2.0*np.exp(x+y)\n",
    "    rf = f1*(1.0+z)/2.0 + fn1*(1.0-z)/2.0\n",
    "    return rf\n",
    "\n",
    "# normal_u = \\nabla u \\dot n, normal derivative of u, only defined on the interface\n",
    "def normal_u(x, y, z):\n",
    "    normal = normal_vector(x, y)\n",
    "    normal_x = normal[:, 0:1]\n",
    "    normal_y = normal[:, 1:2]\n",
    "    u1x = np.cos(x)*np.sin(y)\n",
    "    u1y = np.sin(x)*np.cos(y)\n",
    "    u1 = normal_x*u1x + normal_y*u1y\n",
    "    un1x = np.exp(x+y)\n",
    "    un1y = np.exp(x+y)\n",
    "    un1 = normal_x*un1x + normal_y*un1y\n",
    "    nu = u1*(1.0+z)/2.0 + un1*(1.0-z)/2.0\n",
    "    return nu\n",
    "\n",
    "# normal_vector = normal vector, only defined on the interface\n",
    "def normal_vector(x, y):\n",
    "    dist = np.sqrt((25.0*x)**2 + (4.0*y)**2)\n",
    "    normal_x = 25.0*x/dist\n",
    "    normal_y = 4.0*y/dist\n",
    "    normal = np.hstack((normal_x, normal_y))\n",
    "    return normal\n",
    "\n",
    "def sign_x(x, y):\n",
    "    z = 0.0*x + 1.0\n",
    "    for i in range(len(z)):\n",
    "        dist = np.sqrt((x[i]/0.2)**2+(y[i]/0.5)**2)\n",
    "        if dist < 1.0:\n",
    "            z[i] = -1.0\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BcCubMPzsfB1"
   },
   "outputs": [],
   "source": [
    "def chebyshev_first_kind(dim,n):\n",
    "  a_new=(1.0/n)-1.0\n",
    "  X=[]\n",
    "  x=[]\n",
    "  X=(np.mgrid[[slice(None,n),]*dim])\n",
    "  XX=np.cos(np.pi*(X+0.5)/n)\n",
    "  for i in range(len(X)):\n",
    "    x.append(np.array(XX[i].tolist()).reshape(n**dim,1))\n",
    "  return np.hstack(np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1635237654857,
     "user": {
      "displayName": "林得勝",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10265360023258000577"
     },
     "user_tz": -480
    },
    "id": "G_pNIAfNk-Ea",
    "outputId": "7a3e1dc4-762f-4187-9e3b-5fe99b294160"
   },
   "outputs": [],
   "source": [
    "# number of grid points\n",
    "N_inner = 8\n",
    "\n",
    "# Training points\n",
    "\n",
    "## X_inner: points inside the domain, totally (N_inner-1)**2 points\n",
    "X_inner = chebyshev_first_kind(2, 8)\n",
    "x = X_inner[:,0:1]\n",
    "y = X_inner[:,1:2]\n",
    "z = sign_x(x, y)\n",
    "X_inner = np.hstack((X_inner, z))\n",
    "Rf_inner = rf_u(x, y, z)\n",
    "\n",
    "## X_bd: points on the boundary, totally 4*N_inner points\n",
    "cheby_point = chebyshev_first_kind(1, N_inner)\n",
    "dumy_one = np.ones((N_inner,1))\n",
    "xx1 = np.hstack((cheby_point, -1.0*dumy_one, dumy_one))\n",
    "xx2 = np.hstack((-1.0*dumy_one, cheby_point, dumy_one))\n",
    "xx3 = np.hstack((dumy_one, cheby_point, dumy_one))\n",
    "xx4 = np.hstack((cheby_point, dumy_one, dumy_one))\n",
    "X_bd = np.vstack([xx1, xx2, xx3, xx4])\n",
    "\n",
    "## U_bd: function values on the boundary, totally 4*N_inner points\n",
    "x = X_bd[:,0:1]\n",
    "y = X_bd[:,1:2]\n",
    "z = 0.0*x + 1.0\n",
    "U_bd = exact_u(x, y, z)\n",
    "\n",
    "## X_ij: points on the interior interface, totally 4*N_inner points\n",
    "theta = 2.0*np.pi*lhs(1, 4*N_inner)\n",
    "x_ij = 0.2*np.cos(theta)\n",
    "y_ij = 0.5*np.sin(theta)\n",
    "X_ij = np.hstack([x_ij, y_ij])\n",
    "\n",
    "## normal vector\n",
    "Normal_ij = normal_vector(x_ij, y_ij)\n",
    "\n",
    "## Uj_ij: function jump on the interior interface, totally 4*N_inner points\n",
    "Uj_ij = exact_u(x_ij, y_ij, 0.0*x_ij+1.0) - exact_u(x_ij, y_ij, 0.0*x_ij-1.0)\n",
    "\n",
    "# beta_plus\n",
    "beta_plus = 1.0e-3\n",
    "## Unj_ij: normal jump on the interior interface, totally 4*N_inner points\n",
    "Unj_ij = beta_plus*normal_u(x_ij, y_ij, 0.0*x_ij+1.0) - normal_u(x_ij, y_ij, 0.0*x_ij-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain(\n",
      "  (ln1): Linear(in_features=3, out_features=20, bias=True)\n",
      "  (act1): Sigmoid()\n",
      "  (ln2): Linear(in_features=20, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# one-hidden-layer model\n",
    "num_neuron = 20\n",
    "\n",
    "model = Plain(3, num_neuron, 1).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bd = torch.from_numpy(X_bd).requires_grad_(True).double().to(device)\n",
    "U_bd = torch.from_numpy(U_bd).double().to(device)\n",
    "X_inner = torch.from_numpy(X_inner).requires_grad_(True).double().to(device)\n",
    "Rf_inner = torch.from_numpy(Rf_inner).double().to(device)\n",
    "X_ij = torch.from_numpy(X_ij).requires_grad_(True).double().to(device)\n",
    "Normal_ij = torch.from_numpy(Normal_ij).double().to(device)\n",
    "Uj_ij = torch.from_numpy(Uj_ij).double().to(device)\n",
    "Unj_ij = torch.from_numpy(Unj_ij).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerLBFGS = torch.optim.LBFGS(\n",
    "    model.parameters(), \n",
    "    lr=0.1, \n",
    "    max_iter=50000, \n",
    "    max_eval=50000, \n",
    "    history_size=50,\n",
    "    tolerance_grad=np.finfo(float).eps, \n",
    "    tolerance_change=np.finfo(float).eps,\n",
    "    line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func_lbfgs():\n",
    "    \n",
    "    optimizerLBFGS.zero_grad()\n",
    "    global itera\n",
    "    itera += 1\n",
    "\n",
    "    lossLBFGS = loss(model, X_inner, Rf_inner, X_bd, U_bd, X_ij, Normal_ij, Uj_ij, Unj_ij)\n",
    "   \n",
    "    if itera % 100 == 0:\n",
    "        print('Iter %d, LossLBFGS: %.5e' % (itera, lossLBFGS.item()))\n",
    "    \n",
    "    lossLBFGS.backward(retain_graph = True)\n",
    "    \n",
    "    return lossLBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "itera = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, LossLBFGS: 3.54866e+00\n",
      "Iter 100, LossLBFGS: 6.00207e-03\n",
      "Iter 200, LossLBFGS: 7.31392e-04\n",
      "Iter 300, LossLBFGS: 3.51673e-04\n",
      "Iter 400, LossLBFGS: 1.51307e-04\n",
      "Iter 500, LossLBFGS: 1.21804e-04\n",
      "Iter 600, LossLBFGS: 7.09383e-05\n",
      "Iter 700, LossLBFGS: 5.29806e-05\n",
      "Iter 800, LossLBFGS: 3.36776e-05\n",
      "Iter 900, LossLBFGS: 2.71040e-05\n",
      "Iter 1000, LossLBFGS: 1.60141e-05\n",
      "Iter 1100, LossLBFGS: 1.34688e-05\n",
      "Iter 1200, LossLBFGS: 9.32586e-06\n",
      "Iter 1300, LossLBFGS: 7.75023e-06\n",
      "Iter 1400, LossLBFGS: 4.69914e-06\n",
      "Iter 1500, LossLBFGS: 3.63154e-06\n",
      "Iter 1600, LossLBFGS: 2.81531e-06\n",
      "Iter 1700, LossLBFGS: 2.65135e-06\n",
      "Iter 1800, LossLBFGS: 2.10995e-06\n",
      "Iter 1900, LossLBFGS: 1.51102e-06\n",
      "Iter 2000, LossLBFGS: 1.34624e-06\n",
      "Iter 2100, LossLBFGS: 1.33612e-06\n",
      "Iter 2200, LossLBFGS: 1.33565e-06\n",
      "Iter 2300, LossLBFGS: 1.33528e-06\n",
      "Iter 2400, LossLBFGS: 1.33496e-06\n",
      "Iter 2500, LossLBFGS: 1.33466e-06\n",
      "Iter 2600, LossLBFGS: 1.33438e-06\n",
      "Iter 2700, LossLBFGS: 1.33413e-06\n",
      "Iter 2800, LossLBFGS: 1.33387e-06\n",
      "Iter 2900, LossLBFGS: 1.33362e-06\n",
      "Iter 3000, LossLBFGS: 1.33339e-06\n",
      "Iter 3100, LossLBFGS: 1.33315e-06\n",
      "Iter 3200, LossLBFGS: 1.33293e-06\n",
      "Iter 3300, LossLBFGS: 1.33271e-06\n",
      "Iter 3400, LossLBFGS: 1.33248e-06\n",
      "Iter 3500, LossLBFGS: 1.33227e-06\n",
      "Iter 3600, LossLBFGS: 1.33206e-06\n",
      "Iter 3700, LossLBFGS: 1.33184e-06\n",
      "Iter 3800, LossLBFGS: 1.33163e-06\n",
      "Iter 3900, LossLBFGS: 1.33143e-06\n",
      "Iter 4000, LossLBFGS: 1.33121e-06\n",
      "Iter 4100, LossLBFGS: 1.33101e-06\n",
      "Iter 4200, LossLBFGS: 1.33081e-06\n",
      "Iter 4300, LossLBFGS: 1.33060e-06\n",
      "Iter 4400, LossLBFGS: 1.33040e-06\n",
      "Iter 4500, LossLBFGS: 1.33021e-06\n",
      "Iter 4600, LossLBFGS: 1.33003e-06\n",
      "Iter 4700, LossLBFGS: 1.32984e-06\n",
      "Iter 4800, LossLBFGS: 1.32965e-06\n",
      "Iter 4900, LossLBFGS: 1.32947e-06\n",
      "Iter 5000, LossLBFGS: 1.32929e-06\n",
      "Iter 5100, LossLBFGS: 1.32910e-06\n",
      "Iter 5200, LossLBFGS: 1.32892e-06\n",
      "Iter 5300, LossLBFGS: 1.32874e-06\n",
      "Iter 5400, LossLBFGS: 1.32856e-06\n",
      "Iter 5500, LossLBFGS: 1.32838e-06\n",
      "Iter 5600, LossLBFGS: 1.32820e-06\n",
      "Iter 5700, LossLBFGS: 1.32802e-06\n",
      "Iter 5800, LossLBFGS: 1.32784e-06\n",
      "Iter 5900, LossLBFGS: 1.32766e-06\n",
      "Iter 6000, LossLBFGS: 1.32748e-06\n",
      "Iter 6100, LossLBFGS: 1.32730e-06\n",
      "Iter 6200, LossLBFGS: 1.32712e-06\n",
      "Iter 6300, LossLBFGS: 1.32695e-06\n",
      "Iter 6400, LossLBFGS: 1.32677e-06\n",
      "Iter 6500, LossLBFGS: 1.32659e-06\n",
      "Iter 6600, LossLBFGS: 1.32642e-06\n",
      "Iter 6700, LossLBFGS: 1.32624e-06\n",
      "Iter 6800, LossLBFGS: 1.32607e-06\n",
      "Iter 6900, LossLBFGS: 1.32589e-06\n",
      "Iter 7000, LossLBFGS: 1.32571e-06\n",
      "Iter 7100, LossLBFGS: 1.32554e-06\n",
      "Iter 7200, LossLBFGS: 1.32536e-06\n",
      "Iter 7300, LossLBFGS: 1.32519e-06\n",
      "Iter 7400, LossLBFGS: 1.32501e-06\n",
      "Iter 7500, LossLBFGS: 1.32484e-06\n",
      "Iter 7600, LossLBFGS: 1.32467e-06\n",
      "Iter 7700, LossLBFGS: 1.32449e-06\n",
      "Iter 7800, LossLBFGS: 1.32432e-06\n",
      "Iter 7900, LossLBFGS: 1.32415e-06\n",
      "Iter 8000, LossLBFGS: 1.32397e-06\n",
      "Iter 8100, LossLBFGS: 1.32380e-06\n",
      "Iter 8200, LossLBFGS: 1.32363e-06\n",
      "Iter 8300, LossLBFGS: 1.32345e-06\n",
      "Iter 8400, LossLBFGS: 1.32328e-06\n",
      "Iter 8500, LossLBFGS: 1.32311e-06\n",
      "Iter 8600, LossLBFGS: 1.32293e-06\n",
      "Iter 8700, LossLBFGS: 1.32276e-06\n",
      "Iter 8800, LossLBFGS: 1.32259e-06\n",
      "Iter 8900, LossLBFGS: 1.32242e-06\n",
      "Iter 9000, LossLBFGS: 1.32224e-06\n",
      "Iter 9100, LossLBFGS: 1.32207e-06\n",
      "Iter 9200, LossLBFGS: 1.32190e-06\n",
      "Iter 9300, LossLBFGS: 1.32173e-06\n",
      "Iter 9400, LossLBFGS: 1.32156e-06\n",
      "Iter 9500, LossLBFGS: 1.32139e-06\n",
      "Iter 9600, LossLBFGS: 1.32121e-06\n",
      "Iter 9700, LossLBFGS: 1.32104e-06\n",
      "Iter 9800, LossLBFGS: 1.32087e-06\n",
      "Iter 9900, LossLBFGS: 1.32070e-06\n",
      "Iter 10000, LossLBFGS: 1.32053e-06\n",
      "Iter 10100, LossLBFGS: 1.32036e-06\n",
      "Iter 10200, LossLBFGS: 1.32019e-06\n",
      "Iter 10300, LossLBFGS: 1.32002e-06\n",
      "Iter 10400, LossLBFGS: 1.31984e-06\n",
      "Iter 10500, LossLBFGS: 1.31967e-06\n",
      "Iter 10600, LossLBFGS: 1.31950e-06\n",
      "Iter 10700, LossLBFGS: 1.31933e-06\n",
      "Iter 10800, LossLBFGS: 1.31916e-06\n",
      "Iter 10900, LossLBFGS: 1.31899e-06\n",
      "Iter 11000, LossLBFGS: 1.31882e-06\n",
      "Iter 11100, LossLBFGS: 1.31865e-06\n",
      "Iter 11200, LossLBFGS: 1.31848e-06\n",
      "Iter 11300, LossLBFGS: 1.31831e-06\n",
      "Iter 11400, LossLBFGS: 1.31814e-06\n",
      "Iter 11500, LossLBFGS: 1.31797e-06\n",
      "Iter 11600, LossLBFGS: 1.31780e-06\n",
      "Iter 11700, LossLBFGS: 1.31763e-06\n",
      "Iter 11800, LossLBFGS: 1.31746e-06\n",
      "Iter 11900, LossLBFGS: 1.31729e-06\n",
      "Iter 12000, LossLBFGS: 1.31712e-06\n",
      "Iter 12100, LossLBFGS: 1.31695e-06\n",
      "Iter 12200, LossLBFGS: 1.31678e-06\n",
      "Iter 12300, LossLBFGS: 1.31661e-06\n",
      "Iter 12400, LossLBFGS: 1.31644e-06\n",
      "Iter 12500, LossLBFGS: 1.31627e-06\n",
      "Iter 12600, LossLBFGS: 1.31610e-06\n",
      "Iter 12700, LossLBFGS: 1.31593e-06\n",
      "Iter 12800, LossLBFGS: 1.31576e-06\n",
      "Iter 12900, LossLBFGS: 1.31559e-06\n",
      "Iter 13000, LossLBFGS: 1.31542e-06\n",
      "Iter 13100, LossLBFGS: 1.31525e-06\n",
      "Iter 13200, LossLBFGS: 1.31508e-06\n",
      "Iter 13300, LossLBFGS: 1.31491e-06\n",
      "Iter 13400, LossLBFGS: 1.31475e-06\n",
      "Iter 13500, LossLBFGS: 1.31458e-06\n",
      "Iter 13600, LossLBFGS: 1.31441e-06\n",
      "Iter 13700, LossLBFGS: 1.31424e-06\n",
      "Iter 13800, LossLBFGS: 1.31407e-06\n",
      "Iter 13900, LossLBFGS: 1.31390e-06\n",
      "Iter 14000, LossLBFGS: 1.31373e-06\n",
      "Iter 14100, LossLBFGS: 1.31356e-06\n",
      "Iter 14200, LossLBFGS: 1.31339e-06\n",
      "Iter 14300, LossLBFGS: 1.31323e-06\n",
      "Iter 14400, LossLBFGS: 1.31306e-06\n",
      "Iter 14500, LossLBFGS: 1.31289e-06\n",
      "Iter 14600, LossLBFGS: 1.31272e-06\n",
      "Iter 14700, LossLBFGS: 1.31255e-06\n",
      "Iter 14800, LossLBFGS: 1.31238e-06\n",
      "Iter 14900, LossLBFGS: 1.31221e-06\n",
      "Iter 15000, LossLBFGS: 1.31205e-06\n",
      "Iter 15100, LossLBFGS: 1.31188e-06\n",
      "Iter 15200, LossLBFGS: 1.31171e-06\n",
      "Iter 15300, LossLBFGS: 1.31154e-06\n",
      "Iter 15400, LossLBFGS: 1.31137e-06\n",
      "Iter 15500, LossLBFGS: 1.31120e-06\n",
      "Iter 15600, LossLBFGS: 1.31104e-06\n",
      "Iter 15700, LossLBFGS: 1.31087e-06\n",
      "Iter 15800, LossLBFGS: 1.31070e-06\n",
      "Iter 15900, LossLBFGS: 1.31053e-06\n",
      "Iter 16000, LossLBFGS: 1.31036e-06\n",
      "Iter 16100, LossLBFGS: 1.31020e-06\n",
      "Iter 16200, LossLBFGS: 1.31003e-06\n",
      "Iter 16300, LossLBFGS: 1.30986e-06\n",
      "Iter 16400, LossLBFGS: 1.30969e-06\n",
      "Iter 16500, LossLBFGS: 1.30953e-06\n",
      "Iter 16600, LossLBFGS: 1.30936e-06\n",
      "Iter 16700, LossLBFGS: 1.30919e-06\n",
      "Iter 16800, LossLBFGS: 1.30902e-06\n",
      "Iter 16900, LossLBFGS: 1.30886e-06\n",
      "Iter 17000, LossLBFGS: 1.30869e-06\n",
      "Iter 17100, LossLBFGS: 1.30852e-06\n",
      "Iter 17200, LossLBFGS: 1.30835e-06\n",
      "Iter 17300, LossLBFGS: 1.30819e-06\n",
      "Iter 17400, LossLBFGS: 1.30802e-06\n",
      "Iter 17500, LossLBFGS: 1.30785e-06\n",
      "Iter 17600, LossLBFGS: 1.30768e-06\n",
      "Iter 17700, LossLBFGS: 1.30752e-06\n",
      "Iter 17800, LossLBFGS: 1.30735e-06\n",
      "Iter 17900, LossLBFGS: 1.30718e-06\n",
      "Iter 18000, LossLBFGS: 1.30702e-06\n",
      "Iter 18100, LossLBFGS: 1.30685e-06\n",
      "Iter 18200, LossLBFGS: 1.30668e-06\n",
      "Iter 18300, LossLBFGS: 1.30653e-06\n",
      "Iter 18400, LossLBFGS: 1.30638e-06\n",
      "Iter 18500, LossLBFGS: 1.30623e-06\n",
      "Iter 18600, LossLBFGS: 1.30608e-06\n",
      "Iter 18700, LossLBFGS: 1.30592e-06\n",
      "Iter 18800, LossLBFGS: 1.30577e-06\n",
      "Iter 18900, LossLBFGS: 1.30562e-06\n",
      "Iter 19000, LossLBFGS: 1.30547e-06\n",
      "Iter 19100, LossLBFGS: 1.30532e-06\n",
      "Iter 19200, LossLBFGS: 1.30517e-06\n",
      "Iter 19300, LossLBFGS: 1.30502e-06\n",
      "Iter 19400, LossLBFGS: 1.30487e-06\n",
      "Iter 19500, LossLBFGS: 1.30472e-06\n",
      "Iter 19600, LossLBFGS: 1.30457e-06\n",
      "Iter 19700, LossLBFGS: 1.30442e-06\n",
      "Iter 19800, LossLBFGS: 1.30427e-06\n",
      "Iter 19900, LossLBFGS: 1.30412e-06\n",
      "Iter 20000, LossLBFGS: 1.30397e-06\n",
      "Iter 20100, LossLBFGS: 1.30382e-06\n",
      "Iter 20200, LossLBFGS: 1.30367e-06\n",
      "Iter 20300, LossLBFGS: 1.30352e-06\n",
      "Iter 20400, LossLBFGS: 1.30337e-06\n",
      "Iter 20500, LossLBFGS: 1.30322e-06\n",
      "Iter 20600, LossLBFGS: 1.30307e-06\n",
      "Iter 20700, LossLBFGS: 1.30292e-06\n",
      "Iter 20800, LossLBFGS: 1.30277e-06\n",
      "Iter 20900, LossLBFGS: 1.30262e-06\n",
      "Iter 21000, LossLBFGS: 1.30247e-06\n",
      "Iter 21100, LossLBFGS: 1.30232e-06\n",
      "Iter 21200, LossLBFGS: 1.30217e-06\n",
      "Iter 21300, LossLBFGS: 1.30202e-06\n",
      "Iter 21400, LossLBFGS: 1.30187e-06\n",
      "Iter 21500, LossLBFGS: 1.30172e-06\n",
      "Iter 21600, LossLBFGS: 1.30157e-06\n",
      "Iter 21700, LossLBFGS: 1.30142e-06\n",
      "Iter 21800, LossLBFGS: 1.30127e-06\n",
      "Iter 21900, LossLBFGS: 1.30112e-06\n",
      "Iter 22000, LossLBFGS: 1.30097e-06\n",
      "Iter 22100, LossLBFGS: 1.30082e-06\n",
      "Iter 22200, LossLBFGS: 1.30067e-06\n",
      "Iter 22300, LossLBFGS: 1.30052e-06\n",
      "Iter 22400, LossLBFGS: 1.30037e-06\n",
      "Iter 22500, LossLBFGS: 1.30022e-06\n",
      "Iter 22600, LossLBFGS: 1.30007e-06\n",
      "Iter 22700, LossLBFGS: 1.29992e-06\n",
      "Iter 22800, LossLBFGS: 1.29978e-06\n",
      "Iter 22900, LossLBFGS: 1.29963e-06\n",
      "Iter 23000, LossLBFGS: 1.29948e-06\n",
      "Iter 23100, LossLBFGS: 1.29933e-06\n",
      "Iter 23200, LossLBFGS: 1.29918e-06\n",
      "Iter 23300, LossLBFGS: 1.29903e-06\n",
      "Iter 23400, LossLBFGS: 1.29888e-06\n",
      "Iter 23500, LossLBFGS: 1.29873e-06\n",
      "Iter 23600, LossLBFGS: 1.29858e-06\n",
      "Iter 23700, LossLBFGS: 1.29844e-06\n",
      "Iter 23800, LossLBFGS: 1.29829e-06\n",
      "Iter 23900, LossLBFGS: 1.29814e-06\n",
      "Iter 24000, LossLBFGS: 1.29799e-06\n",
      "Iter 24100, LossLBFGS: 1.29784e-06\n",
      "Iter 24200, LossLBFGS: 1.29769e-06\n",
      "Iter 24300, LossLBFGS: 1.29754e-06\n",
      "Iter 24400, LossLBFGS: 1.29740e-06\n",
      "Iter 24500, LossLBFGS: 1.29725e-06\n",
      "Iter 24600, LossLBFGS: 1.29710e-06\n",
      "Iter 24700, LossLBFGS: 1.29695e-06\n",
      "Iter 24800, LossLBFGS: 1.29680e-06\n",
      "Iter 24900, LossLBFGS: 1.29665e-06\n",
      "Iter 25000, LossLBFGS: 1.29651e-06\n",
      "Iter 25100, LossLBFGS: 1.29636e-06\n",
      "Iter 25200, LossLBFGS: 1.29621e-06\n",
      "Iter 25300, LossLBFGS: 1.29606e-06\n",
      "Iter 25400, LossLBFGS: 1.29591e-06\n",
      "Iter 25500, LossLBFGS: 1.29577e-06\n",
      "Iter 25600, LossLBFGS: 1.29562e-06\n",
      "Iter 25700, LossLBFGS: 1.29547e-06\n",
      "Iter 25800, LossLBFGS: 1.29532e-06\n",
      "Iter 25900, LossLBFGS: 1.29518e-06\n",
      "Iter 26000, LossLBFGS: 1.29503e-06\n",
      "Iter 26100, LossLBFGS: 1.29488e-06\n",
      "Iter 26200, LossLBFGS: 1.29473e-06\n",
      "Iter 26300, LossLBFGS: 1.29458e-06\n",
      "Iter 26400, LossLBFGS: 1.29444e-06\n",
      "Iter 26500, LossLBFGS: 1.29429e-06\n",
      "Iter 26600, LossLBFGS: 1.29414e-06\n",
      "Iter 26700, LossLBFGS: 1.29399e-06\n",
      "Iter 26800, LossLBFGS: 1.29385e-06\n",
      "Iter 26900, LossLBFGS: 1.29370e-06\n",
      "Iter 27000, LossLBFGS: 1.29355e-06\n",
      "Iter 27100, LossLBFGS: 1.29340e-06\n",
      "Iter 27200, LossLBFGS: 1.29326e-06\n",
      "Iter 27300, LossLBFGS: 1.29311e-06\n",
      "Iter 27400, LossLBFGS: 1.29296e-06\n",
      "Iter 27500, LossLBFGS: 1.29282e-06\n",
      "Iter 27600, LossLBFGS: 1.29267e-06\n",
      "Iter 27700, LossLBFGS: 1.29252e-06\n",
      "Iter 27800, LossLBFGS: 1.29237e-06\n",
      "Iter 27900, LossLBFGS: 1.29223e-06\n",
      "Iter 28000, LossLBFGS: 1.29208e-06\n",
      "Iter 28100, LossLBFGS: 1.29193e-06\n",
      "Iter 28200, LossLBFGS: 1.29179e-06\n",
      "Iter 28300, LossLBFGS: 1.29164e-06\n",
      "Iter 28400, LossLBFGS: 1.29149e-06\n",
      "Iter 28500, LossLBFGS: 1.29135e-06\n",
      "Iter 28600, LossLBFGS: 1.29120e-06\n",
      "Iter 28700, LossLBFGS: 1.29105e-06\n",
      "Iter 28800, LossLBFGS: 1.29091e-06\n",
      "Iter 28900, LossLBFGS: 1.29076e-06\n",
      "Iter 29000, LossLBFGS: 1.29061e-06\n",
      "Iter 29100, LossLBFGS: 1.29047e-06\n",
      "Iter 29200, LossLBFGS: 1.29032e-06\n",
      "Iter 29300, LossLBFGS: 1.29017e-06\n",
      "Iter 29400, LossLBFGS: 1.29003e-06\n",
      "Iter 29500, LossLBFGS: 1.28988e-06\n",
      "Iter 29600, LossLBFGS: 1.28973e-06\n",
      "Iter 29700, LossLBFGS: 1.28959e-06\n",
      "Iter 29800, LossLBFGS: 1.28944e-06\n",
      "Iter 29900, LossLBFGS: 1.28929e-06\n",
      "Iter 30000, LossLBFGS: 1.28915e-06\n",
      "Iter 30100, LossLBFGS: 1.28900e-06\n",
      "Iter 30200, LossLBFGS: 1.28885e-06\n",
      "Iter 30300, LossLBFGS: 1.28871e-06\n",
      "Iter 30400, LossLBFGS: 1.28856e-06\n",
      "Iter 30500, LossLBFGS: 1.28842e-06\n",
      "Iter 30600, LossLBFGS: 1.28827e-06\n",
      "Iter 30700, LossLBFGS: 1.28812e-06\n",
      "Iter 30800, LossLBFGS: 1.28798e-06\n",
      "Iter 30900, LossLBFGS: 1.28783e-06\n",
      "Iter 31000, LossLBFGS: 1.28769e-06\n",
      "Iter 31100, LossLBFGS: 1.28754e-06\n",
      "Iter 31200, LossLBFGS: 1.28739e-06\n",
      "Iter 31300, LossLBFGS: 1.28725e-06\n",
      "Iter 31400, LossLBFGS: 1.28710e-06\n",
      "Iter 31500, LossLBFGS: 1.28696e-06\n",
      "Iter 31600, LossLBFGS: 1.28681e-06\n",
      "Iter 31700, LossLBFGS: 1.28667e-06\n",
      "Iter 31800, LossLBFGS: 1.28652e-06\n",
      "Iter 31900, LossLBFGS: 1.28637e-06\n",
      "Iter 32000, LossLBFGS: 1.28623e-06\n",
      "Iter 32100, LossLBFGS: 1.28608e-06\n",
      "Iter 32200, LossLBFGS: 1.28594e-06\n",
      "Iter 32300, LossLBFGS: 1.28579e-06\n",
      "Iter 32400, LossLBFGS: 1.28565e-06\n",
      "Iter 32500, LossLBFGS: 1.28550e-06\n",
      "Iter 32600, LossLBFGS: 1.28535e-06\n",
      "Iter 32700, LossLBFGS: 1.28521e-06\n",
      "Iter 32800, LossLBFGS: 1.28506e-06\n",
      "Iter 32900, LossLBFGS: 1.28492e-06\n",
      "Iter 33000, LossLBFGS: 1.28477e-06\n",
      "Iter 33100, LossLBFGS: 1.28463e-06\n",
      "Iter 33200, LossLBFGS: 1.28448e-06\n",
      "Iter 33300, LossLBFGS: 1.28434e-06\n",
      "Iter 33400, LossLBFGS: 1.28419e-06\n",
      "Iter 33500, LossLBFGS: 1.28405e-06\n",
      "Iter 33600, LossLBFGS: 1.28390e-06\n",
      "Iter 33700, LossLBFGS: 1.28376e-06\n",
      "Iter 33800, LossLBFGS: 1.28361e-06\n",
      "Iter 33900, LossLBFGS: 1.28347e-06\n",
      "Iter 34000, LossLBFGS: 1.28332e-06\n",
      "Iter 34100, LossLBFGS: 1.28318e-06\n",
      "Iter 34200, LossLBFGS: 1.28303e-06\n",
      "Iter 34300, LossLBFGS: 1.28289e-06\n",
      "Iter 34400, LossLBFGS: 1.28274e-06\n",
      "Iter 34500, LossLBFGS: 1.28260e-06\n",
      "Iter 34600, LossLBFGS: 1.28245e-06\n",
      "Iter 34700, LossLBFGS: 1.28231e-06\n",
      "Iter 34800, LossLBFGS: 1.28216e-06\n",
      "Iter 34900, LossLBFGS: 1.28202e-06\n",
      "Iter 35000, LossLBFGS: 1.28187e-06\n",
      "Iter 35100, LossLBFGS: 1.28173e-06\n",
      "Iter 35200, LossLBFGS: 1.28158e-06\n",
      "Iter 35300, LossLBFGS: 1.28144e-06\n",
      "Iter 35400, LossLBFGS: 1.28129e-06\n",
      "Iter 35500, LossLBFGS: 1.28115e-06\n",
      "Iter 35600, LossLBFGS: 1.28100e-06\n",
      "Iter 35700, LossLBFGS: 1.28086e-06\n",
      "Iter 35800, LossLBFGS: 1.28071e-06\n",
      "Iter 35900, LossLBFGS: 1.28057e-06\n",
      "Iter 36000, LossLBFGS: 1.28042e-06\n",
      "Iter 36100, LossLBFGS: 1.28028e-06\n",
      "Iter 36200, LossLBFGS: 1.28013e-06\n",
      "Iter 36300, LossLBFGS: 1.27999e-06\n",
      "Iter 36400, LossLBFGS: 1.27985e-06\n",
      "Iter 36500, LossLBFGS: 1.27970e-06\n",
      "Iter 36600, LossLBFGS: 1.27956e-06\n",
      "Iter 36700, LossLBFGS: 1.27941e-06\n",
      "Iter 36800, LossLBFGS: 1.27927e-06\n",
      "Iter 36900, LossLBFGS: 1.27913e-06\n",
      "Iter 37000, LossLBFGS: 1.27898e-06\n",
      "Iter 37100, LossLBFGS: 1.27884e-06\n",
      "Iter 37200, LossLBFGS: 1.27869e-06\n",
      "Iter 37300, LossLBFGS: 1.27855e-06\n",
      "Iter 37400, LossLBFGS: 1.27840e-06\n",
      "Iter 37500, LossLBFGS: 1.27826e-06\n",
      "Iter 37600, LossLBFGS: 1.27811e-06\n",
      "Iter 37700, LossLBFGS: 1.27797e-06\n",
      "Iter 37800, LossLBFGS: 1.27783e-06\n",
      "Iter 37900, LossLBFGS: 1.27768e-06\n",
      "Iter 38000, LossLBFGS: 1.27754e-06\n",
      "Iter 38100, LossLBFGS: 1.27739e-06\n",
      "Iter 38200, LossLBFGS: 1.27725e-06\n",
      "Iter 38300, LossLBFGS: 1.27711e-06\n",
      "Iter 38400, LossLBFGS: 1.27696e-06\n",
      "Iter 38500, LossLBFGS: 1.27682e-06\n",
      "Iter 38600, LossLBFGS: 1.27667e-06\n",
      "Iter 38700, LossLBFGS: 1.27654e-06\n",
      "Iter 38800, LossLBFGS: 1.27642e-06\n",
      "Iter 38900, LossLBFGS: 1.27630e-06\n",
      "Iter 39000, LossLBFGS: 1.27619e-06\n",
      "Iter 39100, LossLBFGS: 1.27607e-06\n",
      "Iter 39200, LossLBFGS: 1.27595e-06\n",
      "Iter 39300, LossLBFGS: 1.27583e-06\n",
      "Iter 39400, LossLBFGS: 1.27571e-06\n",
      "Iter 39500, LossLBFGS: 1.27559e-06\n",
      "Iter 39600, LossLBFGS: 1.27547e-06\n",
      "Iter 39700, LossLBFGS: 1.27536e-06\n",
      "Iter 39800, LossLBFGS: 1.27524e-06\n",
      "Iter 39900, LossLBFGS: 1.27512e-06\n",
      "Iter 40000, LossLBFGS: 1.27500e-06\n",
      "Iter 40100, LossLBFGS: 1.27488e-06\n",
      "Iter 40200, LossLBFGS: 1.27476e-06\n",
      "Iter 40300, LossLBFGS: 1.27464e-06\n",
      "Iter 40400, LossLBFGS: 1.27453e-06\n",
      "Iter 40500, LossLBFGS: 1.27441e-06\n",
      "Iter 40600, LossLBFGS: 1.27429e-06\n",
      "Iter 40700, LossLBFGS: 1.27417e-06\n",
      "Iter 40800, LossLBFGS: 1.27405e-06\n",
      "Iter 40900, LossLBFGS: 1.27393e-06\n",
      "Iter 41000, LossLBFGS: 1.27382e-06\n",
      "Iter 41100, LossLBFGS: 1.27370e-06\n",
      "Iter 41200, LossLBFGS: 1.27358e-06\n",
      "Iter 41300, LossLBFGS: 1.27346e-06\n",
      "Iter 41400, LossLBFGS: 1.27334e-06\n",
      "Iter 41500, LossLBFGS: 1.27323e-06\n",
      "Iter 41600, LossLBFGS: 1.27311e-06\n",
      "Iter 41700, LossLBFGS: 1.27299e-06\n",
      "Iter 41800, LossLBFGS: 1.27287e-06\n",
      "Iter 41900, LossLBFGS: 1.27275e-06\n",
      "Iter 42000, LossLBFGS: 1.27264e-06\n",
      "Iter 42100, LossLBFGS: 1.27252e-06\n",
      "Iter 42200, LossLBFGS: 1.27240e-06\n",
      "Iter 42300, LossLBFGS: 1.27228e-06\n",
      "Iter 42400, LossLBFGS: 1.27217e-06\n",
      "Iter 42500, LossLBFGS: 1.27205e-06\n",
      "Iter 42600, LossLBFGS: 1.27193e-06\n",
      "Iter 42700, LossLBFGS: 1.27181e-06\n",
      "Iter 42800, LossLBFGS: 1.27169e-06\n",
      "Iter 42900, LossLBFGS: 1.27158e-06\n",
      "Iter 43000, LossLBFGS: 1.27146e-06\n",
      "Iter 43100, LossLBFGS: 1.27134e-06\n",
      "Iter 43200, LossLBFGS: 1.27123e-06\n",
      "Iter 43300, LossLBFGS: 1.27111e-06\n",
      "Iter 43400, LossLBFGS: 1.27099e-06\n",
      "Iter 43500, LossLBFGS: 1.27087e-06\n",
      "Iter 43600, LossLBFGS: 1.27076e-06\n",
      "Iter 43700, LossLBFGS: 1.27064e-06\n",
      "Iter 43800, LossLBFGS: 1.27052e-06\n",
      "Iter 43900, LossLBFGS: 1.27040e-06\n",
      "Iter 44000, LossLBFGS: 1.27029e-06\n",
      "Iter 44100, LossLBFGS: 1.27017e-06\n",
      "Iter 44200, LossLBFGS: 1.27005e-06\n",
      "Iter 44300, LossLBFGS: 1.26994e-06\n",
      "Iter 44400, LossLBFGS: 1.26982e-06\n",
      "Iter 44500, LossLBFGS: 1.26970e-06\n",
      "Iter 44600, LossLBFGS: 1.26958e-06\n",
      "Iter 44700, LossLBFGS: 1.26947e-06\n",
      "Iter 44800, LossLBFGS: 1.26935e-06\n",
      "Iter 44900, LossLBFGS: 1.26923e-06\n",
      "Iter 45000, LossLBFGS: 1.26912e-06\n",
      "Iter 45100, LossLBFGS: 1.26900e-06\n",
      "Iter 45200, LossLBFGS: 1.26888e-06\n",
      "Iter 45300, LossLBFGS: 1.26877e-06\n",
      "Iter 45400, LossLBFGS: 1.26865e-06\n",
      "Iter 45500, LossLBFGS: 1.26853e-06\n",
      "Iter 45600, LossLBFGS: 1.26842e-06\n",
      "Iter 45700, LossLBFGS: 1.26830e-06\n",
      "Iter 45800, LossLBFGS: 1.26818e-06\n",
      "Iter 45900, LossLBFGS: 1.26807e-06\n",
      "Iter 46000, LossLBFGS: 1.26795e-06\n",
      "Iter 46100, LossLBFGS: 1.26783e-06\n",
      "Iter 46200, LossLBFGS: 1.26772e-06\n",
      "Iter 46300, LossLBFGS: 1.26760e-06\n",
      "Iter 46400, LossLBFGS: 1.26748e-06\n",
      "Iter 46500, LossLBFGS: 1.26737e-06\n",
      "Iter 46600, LossLBFGS: 1.26725e-06\n",
      "Iter 46700, LossLBFGS: 1.26713e-06\n",
      "Iter 46800, LossLBFGS: 1.26702e-06\n",
      "Iter 46900, LossLBFGS: 1.26690e-06\n",
      "Iter 47000, LossLBFGS: 1.26679e-06\n",
      "Iter 47100, LossLBFGS: 1.26667e-06\n",
      "Iter 47200, LossLBFGS: 1.26655e-06\n",
      "Iter 47300, LossLBFGS: 1.26644e-06\n",
      "Iter 47400, LossLBFGS: 1.26632e-06\n",
      "Iter 47500, LossLBFGS: 1.26621e-06\n",
      "Iter 47600, LossLBFGS: 1.26609e-06\n",
      "Iter 47700, LossLBFGS: 1.26597e-06\n",
      "Iter 47800, LossLBFGS: 1.26586e-06\n",
      "Iter 47900, LossLBFGS: 1.26574e-06\n",
      "Iter 48000, LossLBFGS: 1.26563e-06\n",
      "Iter 48100, LossLBFGS: 1.26551e-06\n",
      "Iter 48200, LossLBFGS: 1.26539e-06\n",
      "Iter 48300, LossLBFGS: 1.26528e-06\n",
      "Iter 48400, LossLBFGS: 1.26516e-06\n",
      "Iter 48500, LossLBFGS: 1.26505e-06\n",
      "Iter 48600, LossLBFGS: 1.26493e-06\n",
      "Iter 48700, LossLBFGS: 1.26482e-06\n",
      "Iter 48800, LossLBFGS: 1.26470e-06\n",
      "Iter 48900, LossLBFGS: 1.26458e-06\n",
      "Iter 49000, LossLBFGS: 1.26447e-06\n",
      "Iter 49100, LossLBFGS: 1.26435e-06\n",
      "Iter 49200, LossLBFGS: 1.26424e-06\n",
      "Iter 49300, LossLBFGS: 1.26412e-06\n",
      "Iter 49400, LossLBFGS: 1.26401e-06\n",
      "Iter 49500, LossLBFGS: 1.26389e-06\n",
      "Iter 49600, LossLBFGS: 1.26378e-06\n",
      "Iter 49700, LossLBFGS: 1.26366e-06\n",
      "Iter 49800, LossLBFGS: 1.26355e-06\n",
      "Iter 49900, LossLBFGS: 1.26343e-06\n",
      "Iter 50000, LossLBFGS: 1.26331e-06\n",
      "CPU times: user 1min 54s, sys: 242 ms, total: 1min 54s\n",
      "Wall time: 1min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.5487, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.train()\n",
    "\n",
    "lossLBFGS = loss(model, X_inner, Rf_inner, X_bd, U_bd, X_ij, Normal_ij, Uj_ij, Unj_ij)\n",
    "\n",
    "print('Iter %d, LossLBFGS: %.5e' % (itera, lossLBFGS.item()))\n",
    "\n",
    "# Backward and optimize\n",
    "optimizerLBFGS.step(loss_func_lbfgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1635237874244,
     "user": {
      "displayName": "林得勝",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10265360023258000577"
     },
     "user_tz": -480
    },
    "id": "GVMXtOuDXbHX",
    "outputId": "6fc30855-1acb-4981-d798-d175d12968d8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u (absolute inf-norm): 4.762992e-04\n",
      "Error u (absolute 2-norm): 1.360968e-04\n"
     ]
    }
   ],
   "source": [
    "# number of test points\n",
    "N_test = 12800\n",
    "\n",
    "# Error on the interior points\n",
    "X_inn = 2.0*lhs(2, N_test) - 1.0\n",
    "xx = X_inn[:,0:1]\n",
    "yy = X_inn[:,1:2]\n",
    "zz = sign_x(xx, yy)\n",
    "Exact_test = exact_u(xx, yy, zz)\n",
    "X_inn = np.hstack((X_inn, zz))\n",
    "X_inn_torch = torch.tensor(X_inn).double().to(device)\n",
    "u_pred = model(X_inn_torch).detach().cpu().numpy()\n",
    "\n",
    "error = np.absolute(u_pred - Exact_test)\n",
    "\n",
    "error_u_inf = np.linalg.norm(error, np.inf)\n",
    "print('Error u (absolute inf-norm): %e' % (error_u_inf))\n",
    "error_u_2 = np.linalg.norm(error,2)/np.sqrt(N_test)\n",
    "print('Error u (absolute 2-norm): %e' % (error_u_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZcfc6qh2ZTFS9fkQJlNfp",
   "collapsed_sections": [],
   "name": "SINet_poisson_2D_square_ellipse_double.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
