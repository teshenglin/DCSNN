{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Dp5QPfWYia_k"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from functorch import make_functional, vmap, grad, jacrev\n",
    "import functools\n",
    "\n",
    "from pyDOE import lhs\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tK5HlgtUigTf"
   },
   "outputs": [],
   "source": [
    "class Plain(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim , h_dim , out_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear( in_dim , h_dim )\n",
    "        self.act1 =nn.Sigmoid()\n",
    "        self.ln2 = nn.Linear( h_dim , out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.ln2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_Res(func_params, X_inner, Rf_inner):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    grad2_f = (jacrev(grad(f)))(X_inner, func_params)\n",
    "    dudX2 = (torch.diagonal(grad2_f))\n",
    "    \n",
    "    laplace = (dudX2[0] + dudX2[1] + dudX2[2])\n",
    "    \n",
    "    loss_Res = laplace - Rf_inner\n",
    "\n",
    "    return loss_Res.flatten()\n",
    "\n",
    "\n",
    "def compute_loss_b(func_params, X_bd, U_bd):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    u_pred = f(X_bd, func_params)\n",
    "    loss_b = u_pred - U_bd\n",
    "        \n",
    "    return loss_b.flatten()\n",
    "\n",
    "def compute_loss_j(func_params, X_ij, Uj_ij):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    X_ij=X_ij.reshape(len(X_ij), 1)\n",
    "\n",
    "    ij_outer = torch.cat((X_ij[0], X_ij[1], X_ij[2], 1.0+0.0*X_ij[0]), 0)\n",
    "    ij_inner = torch.cat((X_ij[0], X_ij[1], X_ij[2], -1.0+0.0*X_ij[0]), 0)\n",
    "\n",
    "    u_ij_outer = f(ij_outer, func_params)\n",
    "    u_ij_inner = f(ij_inner, func_params)\n",
    "    \n",
    "    ij_pred = u_ij_outer - u_ij_inner\n",
    "    \n",
    "    loss_j = ij_pred - Uj_ij\n",
    "        \n",
    "    return loss_j.flatten()\n",
    "\n",
    "def compute_loss_normal_jump(func_params, X_ij, Unj_ij):\n",
    "\n",
    "    def f(x, func_params):\n",
    "        output = func_model(func_params, x)\n",
    "        return output.squeeze(0)\n",
    "    \n",
    "    X_ij=X_ij.reshape(len(X_ij), 1)\n",
    "    \n",
    "    ij_outer = torch.cat((X_ij[0], X_ij[1], X_ij[2], 1.0+0.0*X_ij[0]), 0)\n",
    "    ij_inner = torch.cat((X_ij[0], X_ij[1], X_ij[2], -1.0+0.0*X_ij[0]), 0)\n",
    "\n",
    "    grad_f_outer = (grad(f))(ij_outer, func_params)\n",
    "    df_outer = (grad_f_outer)\n",
    "    Normal_outer = (df_outer[0]*X_ij[0]/49.0 + df_outer[1]*X_ij[1]/25.0 + df_outer[2]*X_ij[2]/9.0)/torch.sqrt((X_ij[0]/49.0)**2 + (X_ij[1]/25.0)**2 + (X_ij[2]/9.0)**2)\n",
    "    grad_f_inner = (grad(f))(ij_inner, func_params)\n",
    "    df_inner = (grad_f_inner)\n",
    "    Normal_inner = (df_inner[0]*X_ij[0]/49.0 + df_inner[1]*X_ij[1]/25.0 + df_inner[2]*X_ij[2]/9.0)/torch.sqrt((X_ij[0]/49.0)**2 + (X_ij[1]/25.0)**2 + (X_ij[2]/9.0)**2)\n",
    "    \n",
    "    normal_jump_pred = 1.0e-3*Normal_outer - Normal_inner\n",
    "\n",
    "    loss_normal_jump = normal_jump_pred - Unj_ij\n",
    "        \n",
    "    return loss_normal_jump.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Wxr5Jd6B6nPE"
   },
   "outputs": [],
   "source": [
    "def exact_u(X):\n",
    "  z = X[:, 3:4]\n",
    "  # exact_u = exact solution\n",
    "  # if z= 1 -> u1\n",
    "  u1 = np.sin(X[:,0:1])*np.sin(X[:,1:2])*np.sin(X[:,2:3])\n",
    "  # if z=-1 -> fn1\n",
    "  un1 = np.exp(X[:,0:1]+X[:,1:2]+X[:,2:3])\n",
    "  eu = u1*(z+1.0)/2.0 + un1*(1.0-z)/2.0\n",
    "  return eu\n",
    "\n",
    "def rhs_f(x):\n",
    "  # if z= 1 -> f1\n",
    "  f1 = -3.0*np.sin(x[:,0:1])*np.sin(x[:,1:2])*np.sin(x[:,2:3])\n",
    "  # if z=-1 -> fn1\n",
    "  fn1 = 3.0*np.exp(x[:,0:1]+x[:,1:2]+x[:,2:3])\n",
    "  f = f1*(x[:,3:4]+1.0)/2.0 + fn1*(1.0-x[:,3:4])/2.0\n",
    "  return f\n",
    "\n",
    "def normal_u(x0, x1, x2, z):\n",
    "  # exact_u = exact solution\n",
    "  # if z= 1 -> u1\n",
    "  u1 = np.cos(x0)*np.sin(x1)*np.sin(x2)*x0/49.0\\\n",
    "  +np.sin(x0)*np.cos(x1)*np.sin(x2)*x1/25.0\\\n",
    "  +np.sin(x0)*np.sin(x1)*np.cos(x2)*x2/9.0\n",
    "  dist = np.sqrt((x0/49.0)**2 + (x1/25.0)**2 + (x2/9.0)**2)\n",
    "  u1 = u1/dist\n",
    "  # if z=-1 -> fn1\n",
    "  un1 = (x0/49.0+x1/25.0+x2/9.0)*np.exp(x0+x1+x2)\n",
    "  un1 = un1/dist\n",
    "  nu = u1*(z+1.0)/2.0 + un1*(1.0-z)/2.0\n",
    "  return nu\n",
    "\n",
    "def sign_x(X):\n",
    "  z = 0.0*X[:,0:1] + 1.0\n",
    "  for i in range(len(z)):\n",
    "    dist = (X[i,0]/0.7)**2+(X[i,1]/0.5)**2+(X[i,2]/0.3)**2\n",
    "    if dist < 1.0:\n",
    "      z[i] = -1.0\n",
    "  return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_iLQ5IfK1kkV"
   },
   "outputs": [],
   "source": [
    "def chebyshev_first_kind(dim,n):\n",
    "  a_new=(1.0/n)-1.0\n",
    "  X=[]\n",
    "  x=[]\n",
    "  X=(np.mgrid[[slice(None,n),]*dim])\n",
    "  XX=np.cos(np.pi*(X+0.5)/n)\n",
    "  for i in range(len(X)):\n",
    "    x.append(np.array(XX[i].tolist()).reshape(n**dim,1))\n",
    "  return np.hstack(np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXmSiyPzRr-R",
    "outputId": "9e086b80-8627-4dca-e50d-e3266a3cd9b4"
   },
   "outputs": [],
   "source": [
    "d=3\n",
    "\n",
    "# number of grid points\n",
    "N_inner = 216\n",
    "N_bd = 36\n",
    "N_ij = 3*N_bd\n",
    "\n",
    "# Training points\n",
    "\n",
    "## X_inner: points inside the domain, totally (N_inner-1)**2 points\n",
    "X_inner = chebyshev_first_kind(d, 6)\n",
    "z_inner = sign_x(X_inner)\n",
    "X_inner = np.hstack([X_inner, z_inner])\n",
    "Rf_inner = rhs_f(X_inner)\n",
    "Rf_inner = Rf_inner.reshape(N_inner, 1)\n",
    "\n",
    "## X_bd: points at the boundary, totally 2*d*N_bd points\n",
    "xx = lhs(d-1, N_bd)*2.0-1.0\n",
    "xx = np.hstack((-1.0*np.ones((N_bd,1)), xx))\n",
    "yy = lhs(d-1, N_bd)*2.0-1.0\n",
    "yy = np.hstack((np.ones((N_bd,1)), yy))\n",
    "X_bd = np.vstack([xx, yy])\n",
    "\n",
    "for i in range(1, d-1):\n",
    "  xx1 = lhs(i, N_bd)*2.0-1.0\n",
    "  xx2 = lhs(d-1-i, N_bd)*2.0-1.0\n",
    "  xx = np.hstack((xx1, -1.0*np.ones((N_bd,1)), xx2))\n",
    "  yy1 = lhs(i, N_bd)*2.0-1.0\n",
    "  yy2 = lhs(d-1-i, N_bd)*2.0-1.0\n",
    "  yy = np.hstack((yy1, np.ones((N_bd,1)), yy2))\n",
    "  X_bd = np.vstack([X_bd, xx, yy])\n",
    "\n",
    "xx = lhs(d-1, N_bd)*2.0-1.0\n",
    "xx = np.hstack((xx, -1.0*np.ones((N_bd,1))))\n",
    "yy = lhs(d-1, N_bd)*2.0-1.0\n",
    "yy = np.hstack((yy, np.ones((N_bd,1))))\n",
    "X_bd = np.vstack([X_bd, xx, yy])\n",
    "X_bd = np.hstack((X_bd, 1.0+0.0*X_bd[:,0:1]))\n",
    "\n",
    "## U_bd: function values at the boundary, totally 2*d*N_bd points\n",
    "U_bd = exact_u(np.hstack([X_bd, 0.0*X_bd[:,0:1]+1.0]))\n",
    "U_bd = U_bd.reshape(2*d*N_bd, 1)\n",
    "\n",
    "## X_ij: points at the interior interface, totally N_bd points\n",
    "r_ij = lhs(1,N_ij)*2.0*np.pi\n",
    "p_ij = lhs(1,N_ij)*np.pi\n",
    "x0_ij = 0.7*np.cos(r_ij)*np.sin(p_ij)\n",
    "x1_ij = 0.5*np.sin(r_ij)*np.sin(p_ij)\n",
    "x2_ij = 0.3*np.cos(p_ij)\n",
    "X_ij  = np.hstack([x0_ij, x1_ij, x2_ij])\n",
    "\n",
    "## Uj_ij: function jump at the interior interface, totally N_bd points\n",
    "Uj_ij = exact_u(np.hstack([X_ij, 0.0*X_ij[:,0:1]+1.0])) - exact_u(np.hstack([X_ij, 0.0*X_ij[:,0:1]-1.0]))\n",
    "Uj_ij = Uj_ij.reshape(N_ij, 1)\n",
    "\n",
    "# beta_plus\n",
    "beta_plus = 1.0e-3\n",
    "## Unj_ij: normal jump at the interior interface, totally N_bd points\n",
    "Unj_ij = beta_plus*normal_u(x0_ij, x1_ij, x2_ij, 0.0*x0_ij+1.0) - normal_u(x0_ij, x1_ij, x2_ij, 0.0*x0_ij-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain(\n",
      "  (ln1): Linear(in_features=4, out_features=30, bias=True)\n",
      "  (act1): Sigmoid()\n",
      "  (ln2): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# single-layer model\n",
    "model = Plain(4, 30, 1).to(device)\n",
    "print(model)\n",
    "\n",
    "# Make model a functional\n",
    "func_model, func_params = make_functional(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bd = torch.from_numpy(X_bd).requires_grad_(True).double().to(device)\n",
    "U_bd = torch.from_numpy(U_bd).double().to(device)\n",
    "X_inner = torch.from_numpy(X_inner).requires_grad_(True).double().to(device)\n",
    "Rf_inner = torch.from_numpy(Rf_inner).double().to(device)\n",
    "X_ij = torch.from_numpy(X_ij).requires_grad_(True).double().to(device)\n",
    "Uj_ij = torch.from_numpy(Uj_ij).double().to(device)\n",
    "Unj_ij = torch.from_numpy(Unj_ij).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_iter = 2000\n",
    "mu_update = 2 # update \\mu every mu_update iterations\n",
    "div_factor = 1.3 # \\mu <- \\mu/div_factor when loss decreases\n",
    "mul_factor = 3 # \\mu <- mul_factor*\\mu when loss incerases\n",
    "\n",
    "mu = 10**5\n",
    "loss_sum_old = 10**5\n",
    "itera = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aChmjTLTRsNt",
    "outputId": "84649686-7f62-4ab4-e3a9-e7137d89d2bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, Loss_Res: 4.32054e+00, mu: 7.69231e+04\n",
      "Iter 101, Loss_Res: 3.67593e-01, mu: 1.54486e-01\n",
      "Iter 201, Loss_Res: 1.41829e-05, mu: 4.71901e-06\n",
      "Iter 301, Loss_Res: 1.71287e-07, mu: 1.97817e-06\n",
      "Iter 401, Loss_Res: 5.00307e-08, mu: 1.26127e-05\n",
      "Iter 501, Loss_Res: 2.03807e-08, mu: 5.28715e-06\n",
      "Iter 601, Loss_Res: 1.19812e-08, mu: 8.64370e-06\n",
      "Iter 701, Loss_Res: 7.60825e-09, mu: 1.41312e-05\n",
      "Iter 801, Loss_Res: 3.18604e-09, mu: 5.92369e-06\n",
      "Iter 901, Loss_Res: 1.29649e-09, mu: 2.48317e-06\n",
      "Iter 1001, Loss_Res: 9.94227e-10, mu: 1.04093e-06\n",
      "Iter 1101, Loss_Res: 8.45260e-10, mu: 4.36349e-07\n",
      "Iter 1201, Loss_Res: 7.53052e-10, mu: 7.13366e-07\n",
      "Iter 1301, Loss_Res: 6.71532e-10, mu: 2.99038e-07\n",
      "Iter 1401, Loss_Res: 6.16106e-10, mu: 4.88882e-07\n",
      "Iter 1501, Loss_Res: 5.71979e-10, mu: 2.04936e-07\n",
      "Iter 1601, Loss_Res: 5.26540e-10, mu: 8.59077e-08\n",
      "Iter 1701, Loss_Res: 4.78344e-10, mu: 1.40446e-07\n",
      "Iter 1801, Loss_Res: 4.35230e-10, mu: 2.29609e-07\n",
      "Iter 1901, Loss_Res: 4.12652e-10, mu: 3.75377e-07\n",
      "Iter 2001, Loss_Res: 3.95248e-10, mu: 1.57355e-07\n",
      "CPU times: user 1min 16s, sys: 2min 30s, total: 3min 46s\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for step in range(LM_iter+1):\n",
    "    # Put into loss functional to get L_vec\n",
    "    L_vec_res = vmap(compute_loss_Res, (None, 0, 0))(func_params, X_inner, Rf_inner)\n",
    "    L_vec_b = vmap(compute_loss_b, (None, 0, 0))(func_params, X_bd, U_bd)\n",
    "    L_vec_j = vmap(compute_loss_j, (None, 0, 0))(func_params, X_ij, Uj_ij)\n",
    "    L_vec_nj = vmap(compute_loss_normal_jump, (None, 0, 0))(func_params, X_ij, Unj_ij)\n",
    "\n",
    "    L_vec_res = L_vec_res/np.sqrt(N_inner)\n",
    "    L_vec_b = L_vec_b/np.sqrt(N_inner)\n",
    "    L_vec_j = L_vec_j/np.sqrt(3.0*N_bd)\n",
    "    L_vec_nj = L_vec_nj/np.sqrt(3.0*N_bd)\n",
    "    loss = torch.sum(L_vec_res**2) + torch.sum(L_vec_b**2) + torch.sum(L_vec_j**2) + torch.sum(L_vec_nj**2)\n",
    "\n",
    "    # Consturct J for domain points\n",
    "    # (None, 0 ,0): func_params: no batch. data_d: batch wrt shape[0] (data[i, :]). force_value: batch wrt shape[0] (force_value[i,:])\n",
    "    \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_Res), (None, 0, 0))(func_params, X_inner, Rf_inner)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_res = g.view(len(g), -1) if cnt == 0 else torch.hstack([J_d_res, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "    \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_b), (None, 0, 0))(func_params, X_bd, U_bd)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_b = g.view(len(g), -1) if cnt == 0 else torch.hstack([J_d_b, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "        \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_j), (None, 0, 0))(func_params, X_ij, Uj_ij)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_j = g.view(len(g), -1) if cnt == 0 else torch.hstack([J_d_j, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "        \n",
    "    per_sample_grads = vmap(jacrev(compute_loss_normal_jump), (None, 0, 0))(func_params, X_ij, Unj_ij)\n",
    "    cnt = 0\n",
    "    for g in per_sample_grads: \n",
    "        g = g.detach()\n",
    "        J_d_nj = g.contiguous().view(len(g), -1) if cnt == 0 else torch.hstack([J_d_nj, g.view(len(g), -1)])\n",
    "        cnt = 1\n",
    "\n",
    "    # cat J_d and J_b into J\n",
    "    J_mat = torch.cat((J_d_res, J_d_b, J_d_j, J_d_nj))\n",
    "    L_vec = torch.cat((L_vec_res, L_vec_b, L_vec_j, L_vec_nj))\n",
    "\n",
    "    # update lambda\n",
    "    I = torch.eye((J_mat.shape[1])).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        J_product = J_mat.t()@J_mat\n",
    "        rhs = -J_mat.t()@L_vec\n",
    "        with torch.no_grad():\n",
    "            dp = torch.linalg.solve(J_product + mu*I, rhs)\n",
    "\n",
    "        # update parameters\n",
    "        cnt=0\n",
    "        for p in func_params:\n",
    "            mm=torch.Tensor([p.shape]).tolist()[0]\n",
    "            num=int(functools.reduce(lambda x,y:x*y,mm,1))\n",
    "            p+=dp[cnt:cnt+num].reshape(p.shape)\n",
    "            cnt+=num\n",
    "\n",
    "        itera += 1\n",
    "        if step % mu_update == 0:\n",
    "            #if loss_sum_check < loss_sum_old:\n",
    "            if loss < loss_sum_old:\n",
    "                mu = max(mu/div_factor, 10**(-9))\n",
    "            else:\n",
    "                mu = min(mul_factor*mu, 10**(8))\n",
    "            loss_sum_old = loss\n",
    "                \n",
    "        if step%100 == 0:\n",
    "            print(\n",
    "                    'Iter %d, Loss_Res: %.5e, mu: %.5e' % (itera, loss.item(), mu)\n",
    "                )\n",
    "\n",
    "        if step == LM_iter or loss.item()<10**(-12):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQTHAR0ARsQN",
    "outputId": "e300784e-36dc-40d6-f41f-380b6074b177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u (absolute inf-norm): 3.215130e-05\n",
      "Error u (absolute 2-norm): 2.480416e-06\n"
     ]
    }
   ],
   "source": [
    "N_test = 2000\n",
    "\n",
    "X_inn = lhs(d, N_test)*2.0-1.0\n",
    "z_inn = sign_x(X_inn)\n",
    "X_inn = np.hstack([X_inn, z_inn])\n",
    "Exact_test = exact_u(X_inn)\n",
    "Exact_test = Exact_test.reshape((N_test,1))\n",
    "X_inn_torch = torch.tensor(X_inn).double().to(device)\n",
    "u_pred = func_model(func_params, X_inn_torch).detach().cpu().numpy()\n",
    "\n",
    "error = np.absolute(u_pred - Exact_test)\n",
    "\n",
    "error_u_inf = np.linalg.norm(error, np.inf)\n",
    "print('Error u (absolute inf-norm): %e' % (error_u_inf))\n",
    "error_u_2 = np.linalg.norm(error,2)/np.sqrt(N_test)\n",
    "print('Error u (absolute 2-norm): %e' % (error_u_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SINet_poisson_3D_cub_elipse_double.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
